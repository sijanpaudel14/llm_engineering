{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1KSMxOCprsl1QRpt_Rq0UqCAyMtPqDQYx","timestamp":1758446111396}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Create meeting minutes from an Audio file\n","\n","I downloaded some Denver City Council meeting minutes and selected a portion of the meeting for us to transcribe. You can download it here:  \n","https://drive.google.com/file/d/1N_kpSojRR5RYzupz6nqM8hMSoEF_R7pU/view?usp=sharing\n","\n","If you'd rather work with the original data, the HuggingFace dataset is [here](https://huggingface.co/datasets/huuuyeah/meetingbank) and the audio can be downloaded [here](https://huggingface.co/datasets/huuuyeah/MeetingBank_Audio/tree/main).\n","\n","The goal of this product is to use the Audio to generate meeting minutes, including actions.\n","\n","For this project, you can either use the Denver meeting minutes, or you can record something of your own!\n"],"metadata":{"id":"It89APiAtTUF"}},{"cell_type":"markdown","source":["## Again - please note: 2 important pro-tips for using Colab:\n","\n","**Pro-tip 1:**\n","\n","The top of every colab has some pip installs. You may receive errors from pip when you run this, such as:\n","\n","> gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n","\n","These pip compatibility errors can be safely ignored; and while it's tempting to try to fix them by changing version numbers, that will actually introduce real problems!\n","\n","**Pro-tip 2:**\n","\n","In the middle of running a Colab, you might get an error like this:\n","\n","> Runtime error: CUDA is required but not available for bitsandbytes. Please consider installing [...]\n","\n","This is a super-misleading error message! Please don't try changing versions of packages...\n","\n","This actually happens because Google has switched out your Colab runtime, perhaps because Google Colab was too busy. The solution is:\n","\n","1. Kernel menu >> Disconnect and delete runtime\n","2. Reload the colab from fresh and Edit menu >> Clear All Outputs\n","3. Connect to a new T4 using the button at the top right\n","4. Select \"View resources\" from the menu on the top right to confirm you have a GPU\n","5. Rerun the cells in the colab, from the top down, starting with the pip installs\n","\n","And all should work great - otherwise, ask me!"],"metadata":{"id":"sJPSCwPX3MOV"}},{"cell_type":"code","source":["!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n","!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 openai"],"metadata":{"id":"f2vvgnFpHpID","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758453288448,"user_tz":-345,"elapsed":243537,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}},"outputId":"0e72c0cb-fbeb-4206-f648-44e94855f83e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.2/908.2 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m131.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.6/336.6 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","execution_count":1,"metadata":{"id":"FW8nl3XRFrz0","executionInfo":{"status":"ok","timestamp":1758457509533,"user_tz":-345,"elapsed":13927,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}}},"outputs":[],"source":["# imports\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","import os\n","import requests\n","from IPython.display import Markdown, display, update_display\n","from openai import OpenAI\n","from google.colab import drive\n","from huggingface_hub import login\n","from google.colab import userdata\n","from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n","import torch"]},{"cell_type":"code","source":["# Constants\n","\n","AUDIO_MODEL = \"whisper-1\"\n","LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""],"metadata":{"id":"q3D1_T0uG_Qh","executionInfo":{"status":"ok","timestamp":1758457509536,"user_tz":-345,"elapsed":1,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hu3tg_3-_Cqa","executionInfo":{"status":"ok","timestamp":1758457511152,"user_tz":-345,"elapsed":1615,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}},"outputId":"9e0fa207-7816-44c3-aed0-c9f2b71f85e0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from google.colab import userdata\n","api_key=userdata.get('GOOGLE_API_KEY')"],"metadata":{"id":"7XbYncNxLwx5","executionInfo":{"status":"ok","timestamp":1758459511618,"user_tz":-345,"elapsed":1743,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["!ls \"/content/drive/MyDrive/Colab Notebooks\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sBLvZrHfF--m","executionInfo":{"status":"ok","timestamp":1758457517940,"user_tz":-345,"elapsed":15,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}},"outputId":"70de957b-be9a-44b3-e140-28530bb2672d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":[" 00_pytorch_fundamentals_vide.ipynb\n","'Copy of Copy of 😡🤢😱😊😐😔😲 Emotion Detection'\n","'Copy of 😡🤢😱😊😐😔😲 Emotion Detection'\n","'Copy of tool-calling-in-langchain.ipynb'\n","'Copy of tools-in-langchain.ipynb'\n","'Copy of Week 3 Day 3 - tokenizers.ipynb'\n","'Copy of Week 3 Day 4 - models.ipynb'\n","'Copy of Week 3 Day 5 - Meeting Minutes product.ipynb'\n"," denver_extract.mp3\n"," langchain-aam-zindagi.ipynb\n"," langchain-retrievers.ipynb\n","'next word predictor.ipynb'\n"," rag-using-langchain.ipynb\n"," tensor_ops.ipynb\n"," Tensors.ipynb\n"," Untitled\n"," Untitled0.ipynb\n"," Untitled1.ipynb\n"," Untitled2.ipynb\n"," Untitled3.ipynb\n"," vector-stores.ipynb\n","'week 3 day 2 - pipelines.ipynb'\n"]}]},{"cell_type":"code","source":["# New capability - connect this Colab to my Google Drive\n","# See immediately below this for instructions to obtain denver_extract.mp3\n","\n","drive.mount(\"/content/drive\")\n","audio_filename = \"/content/drive/MyDrive/Colab Notebooks/denver_extract.mp3\""],"metadata":{"id":"Es9GkQ0FGCMt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758457580455,"user_tz":-345,"elapsed":1767,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}},"outputId":"0b8e2b16-0dba-42a7-a723-f5492aeff144"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["audio_filename"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"njHv6bDxOqog","executionInfo":{"status":"ok","timestamp":1758457582760,"user_tz":-345,"elapsed":42,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}},"outputId":"94b833d5-dca1-4de5-fb74-6db6e362540b"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Colab Notebooks/denver_extract.mp3'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["# Download denver_extract.mp3\n","\n","You can either use the same file as me, the extract from Denver city council minutes, or you can try your own..\n","\n","If you want to use the same as me, then please download my extract here, and put this on your Google Drive:  \n","https://drive.google.com/file/d/1N_kpSojRR5RYzupz6nqM8hMSoEF_R7pU/view?usp=sharing\n"],"metadata":{"id":"HTl3mcjyzIEE"}},{"cell_type":"code","source":["# Sign in to HuggingFace Hub\n","\n","hf_token = userdata.get('HF_TOKEN')\n","login(hf_token, add_to_git_credential=True)"],"metadata":{"id":"xYW8kQYtF-3L","executionInfo":{"status":"ok","timestamp":1758455568968,"user_tz":-345,"elapsed":6860,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# Sign in to OpenAI using Secrets in Colab\n","\n","openai_api_key = userdata.get('OPENAI_API_KEY')\n","openai = OpenAI(api_key=openai_api_key)"],"metadata":{"id":"qP6OB2OeGC2C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use the Whisper OpenAI model to convert the Audio to Text\n","# If you'd prefer to use an Open Source model, class student Youssef has contributed an open source version\n","# which I've added to the bottom of this colab\n","\n","audio_file = open(audio_filename, \"rb\")\n","transcription = openai.audio.transcriptions.create(model=AUDIO_MODEL, file=audio_file, response_format=\"text\")\n","print(transcription)"],"metadata":{"id":"GMShdVGlGGr4","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"error","timestamp":1758455864967,"user_tz":-345,"elapsed":15,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}},"outputId":"ae423061-b074-470b-e7c5-805d944fa8ae"},"execution_count":23,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'openai' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-745770441.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maudio_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtranscription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAUDIO_MODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'openai' is not defined"]}]},{"cell_type":"code","source":["system_message = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n","user_prompt = f\"Below is an extract transcript of a Denver council meeting. Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\\n{transcribed_text}\"\n","\n","messages = [\n","    {\"role\": \"system\", \"content\": system_message},\n","    {\"role\": \"user\", \"content\": user_prompt}\n","  ]\n"],"metadata":{"id":"piEMmcSfMH-O","executionInfo":{"status":"ok","timestamp":1758457058628,"user_tz":-345,"elapsed":4,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    bnb_4bit_quant_type=\"nf4\"\n",")"],"metadata":{"id":"UcRKUgcxMew6","executionInfo":{"status":"ok","timestamp":1758457049562,"user_tz":-345,"elapsed":6,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n","tokenizer.pad_token = tokenizer.eos_token\n","inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n","streamer = TextStreamer(tokenizer)\n","model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n","outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)"],"metadata":{"id":"6CujZRAgMimy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = tokenizer.decode(outputs[0])"],"metadata":{"id":"102tdU_3Peam"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(Markdown(response))"],"metadata":{"id":"KlomN6CwMdoN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Student contribution\n","\n","Student Emad S. has made this powerful variation that uses `TextIteratorStreamer` to stream back results into a Gradio UI, and takes advantage of background threads for performance! I'm sharing it here if you'd like to take a look at some very interesting work. Thank you, Emad!\n","\n","https://colab.research.google.com/drive/1Ja5zyniyJo5y8s1LKeCTSkB2xyDPOt6D"],"metadata":{"id":"kuxYecT2QDQ9"}},{"cell_type":"markdown","source":["## Alternative implementation\n","\n","Class student Youssef has contributed this variation in which we use an open-source model to transcribe the meeting Audio.\n","\n","Thank you Youssef!"],"metadata":{"id":"AU3uAEyU3a-o"}},{"cell_type":"code","source":["AUDIO_MODEL = \"openai/whisper-medium\"\n","speech_model = AutoModelForSpeechSeq2Seq.from_pretrained(AUDIO_MODEL, torch_dtype=torch.float16, low_cpu_mem_usage=True, use_safetensors=True)\n","speech_model.to('cuda')\n","processor = AutoProcessor.from_pretrained(AUDIO_MODEL)\n","\n","pipe = pipeline(\n","    \"automatic-speech-recognition\",\n","    model=speech_model,\n","    tokenizer=processor.tokenizer,\n","    feature_extractor=processor.feature_extractor,\n","    torch_dtype=torch.float16,\n","    device='cuda',\n",")"],"metadata":{"id":"HdQnWEzW3lzP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use the Whisper OpenAI model to convert the Audio to Text\n","result = pipe(audio_filename)"],"metadata":{"id":"nrQjKtD53omJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transcription = result[\"text\"]\n","print(transcription)"],"metadata":{"id":"G_XSljOY3tDf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Of course. Here is the equivalent code using the latest LangChain libraries.\n","\n","The main changes involve replacing the manual `transformers` model loading and generation loop with LangChain's `HuggingFacePipeline` and using LangChain Expression Language (LCEL) to create a clear, composable chain.\n","\n","### Key LangChain Concepts Used:\n","\n","*   **`langchain-huggingface`**: Provides the `HuggingFacePipeline` class, a seamless way to integrate quantized Hugging Face models into LangChain.\n","*   **`langchain-openai`**: (Implicitly used) LangChain integrations for OpenAI models. The original script's direct use of the `openai` SDK for transcription is already efficient and clear, so we'll keep it.\n","*   **Message Objects**: Using `SystemMessage` and `HumanMessage` for cleaner and more structured prompt management.\n","*   **LCEL (LangChain Expression Language)**: The `|` (pipe) operator is used to chain components together. The flow `prompt | model | parser` is a standard and powerful LangChain pattern.\n","*   **`.stream()`**: The chain's `.stream()` method is the direct equivalent of using a `TextStreamer`, providing an iterable of output chunks for a real-time effect.\n"],"metadata":{"id":"mO2BOzTj0CA9"}},{"cell_type":"code","source":["!pip install langchain_google_genai langchain_huggingface"],"metadata":{"id":"eJUkXa980fg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# imports\n","import sys\n","import os\n","import json\n","from dotenv import load_dotenv\n","from openai import OpenAI\n","import gradio as gr\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n","from langchain_core.tools import tool"],"metadata":{"id":"wfYnjZjL0Y4f","executionInfo":{"status":"ok","timestamp":1758453031835,"user_tz":-345,"elapsed":1520,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["\n","\n","### Updated LangChain Code\n","# 1. INSTALLS\n","# Make sure to install the updated langchain packages\n","# !pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n","# !pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0\n","# !pip install -q openai langchain langchain-huggingface\n","\n","# 2. IMPORTS\n","import os\n","import torch\n","from openai import OpenAI\n","from google.colab import drive, userdata\n","from huggingface_hub import login\n","from IPython.display import Markdown, display\n","from transformers import BitsAndBytesConfig\n","\n","# LangChain specific imports\n","from langchain_huggingface import HuggingFacePipeline\n","from langchain_core.messages import SystemMessage, HumanMessage\n","from langchain_core.output_parsers import StrOutputParser\n","\n","\n","# 3. CONSTANTS AND SETUP\n","AUDIO_MODEL = \"whisper-1\"\n","LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n","\n","# 4. AUTHENTICATION\n","# Sign in to HuggingFace Hub\n","hf_token = userdata.get('HF_TOKEN')\n","login(hf_token, add_to_git_credential=True)\n","\n","\n","print(\"Transcription complete.\")\n","print(\"-\" * 20)\n","# print(transcription) # Uncomment to see the full transcript\n","\n","\n","# ==============================================================================\n","# 6. LANGCHAIN IMPLEMENTATION\n","# ==============================================================================\n","\n","# Define the prompt using LangChain message objects\n","system_message = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n","user_prompt = f\"Below is an extract transcript of a Denver council meeting. Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\\n{transcription}\"\n","\n","messages = [\n","    SystemMessage(content=system_message),\n","    HumanMessage(content=user_prompt)\n","]\n","\n","# Define the quantization configuration\n","quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    bnb_4bit_quant_type=\"nf4\"\n",")\n","\n","# Create the LangChain HuggingFacePipeline\n","# This class handles loading the model and tokenizer, and setting up the text-generation pipeline\n","llm = HuggingFacePipeline.from_model_id(\n","    model_id=LLAMA,\n","    task=\"text-generation\",\n","    model_kwargs={\"quantization_config\": quant_config},\n","    device_map=\"auto\",\n","    pipeline_kwargs={\"max_new_tokens\": 2000},\n",")\n","\n","# Create the LCEL chain by piping the components together\n","# The StrOutputParser ensures the output is a clean string\n","chain = llm | StrOutputParser()\n","\n","# Stream the response and collect the full text\n","print(\"Generating meeting minutes...\\n\")\n","full_response = \"\"\n","for chunk in chain.stream(messages):\n","    print(chunk, end=\"\", flush=True)\n","    full_response += chunk\n","\n","print(\"\\n\\n\" + \"=\"*50 + \"\\nGeneration complete.\\n\" + \"=\"*50)\n","\n","# Display the final, complete response as formatted Markdown\n","display(Markdown(full_response))\n","\n","\n","# ==============================================================================\n","# The alternative open-source transcription part can remain the same\n","# as it uses the `transformers` library directly for a specific task.\n","# =============================================================================="],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"id":"af_Qj9Xmz-ko","executionInfo":{"status":"error","timestamp":1758457611002,"user_tz":-345,"elapsed":1410,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}},"outputId":"1286582f-0db7-45ee-8cb8-287b1f3d79fb"},"execution_count":16,"outputs":[{"output_type":"error","ename":"SecretNotFoundError","evalue":"Secret OPENAI_API_KEY does not exist.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2720917926.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Sign in to OpenAI using Secrets in Colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mopenai_api_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'OPENAI_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mopenai\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopenai_api_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mSecretNotFoundError\u001b[0m: Secret OPENAI_API_KEY does not exist."]}]},{"cell_type":"code","source":["# @title\n","import base64\n","from io import BytesIO\n","from pydub import AudioSegment\n","from pydub.playback import play\n","from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from google.api_core.exceptions import ResourceExhausted, GoogleAPICallError\n","from IPython.display import Image, display, Audio, Markdown\n","import mimetypes\n","\n","# Assuming key_utils.py exists and works\n","# from key_utils import get_next_key\n","\n","import langchain_google_genai.chat_models as chat_mod\n","\n","class LLMHandler:\n","    # --- Model Constants ---\n","    # Using modern, flexible models that support conversation history is recommended\n","    TEXT_MODEL = \"gemini-2.0-flash\"\n","    AUDIO_MODEL = \"gemini-2.5-flash-preview-tts\"\n","    IMAGE_MODEL = \"gemini-2.0-flash-preview-image-generation\"\n","    def __init__(self, system_message=\"You are a helpful assistant.\"):\n","        self.conversation_history = []\n","        self.system_message = system_message\n","        self._original_chat_with_retry = chat_mod._chat_with_retry\n","\n"," # --- Patch retry ---\n","    def _patch_retry(self, enable_retry: False):\n","        \"\"\"Enable or disable internal retry inside LangChain/Google API\"\"\"\n","        if not enable_retry:\n","            def no_retry_chat_with_retry(**kwargs):\n","                generation_method = kwargs.pop(\"generation_method\")\n","                metadata = kwargs.pop(\"metadata\", None)\n","                return generation_method(\n","                    request=kwargs.get(\"request\"),\n","                    retry=None,\n","                    timeout=None,\n","                    metadata=metadata\n","                )\n","            chat_mod._chat_with_retry = no_retry_chat_with_retry\n","        else:\n","            chat_mod._chat_with_retry = self._original_chat_with_retry\n","\n","        # --- Helper: extract base64 image ---\n","    def _get_image_base64(self, response):\n","        for block in response.content:\n","            if isinstance(block, dict) and \"image_url\" in block:\n","                return block[\"image_url\"][\"url\"].split(\",\")[-1]\n","        return None\n","\n","\n","\n","    # --- REFACTORED AND FIXED: Message Building ---\n","    def _build_messages(self, user_input, task_type=\"text\"):\n","        \"\"\"\n","        Correctly builds the message list for the API, preserving history for all task types.\n","        \"\"\"\n","        messages = []\n","\n","        # FIX: Only add system message for conversational tasks that support it.\n","        if task_type in [\"text\", \"tool\"]:\n","            messages.append(SystemMessage(content=self.system_message))\n","\n","        # FIX: Convert the entire history correctly for the model.\n","        for h in self.conversation_history:\n","            role = h.get(\"role\")\n","            content = h.get(\"content\")\n","            if role == \"user\":\n","                messages.append(HumanMessage(content=content))\n","            elif role == \"assistant\":\n","                messages.append(AIMessage(content=content))\n","\n","        # Always add the latest user input.\n","        messages.append(HumanMessage(content=user_input))\n","        return messages\n","\n","    # --- REFACTORED AND FIXED: History Updating ---\n","    def _update_history(self, user_input, assistant_response_content):\n","        \"\"\"\n","        Correctly updates the internal history with the latest turn.\n","        \"\"\"\n","        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n","        # FIX: Always store the actual content from the assistant.\n","        self.conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response_content})\n","\n","    def clear_history(self):\n","        \"\"\"Helper to reset the conversation.\"\"\"\n","        self.conversation_history = []\n","        print(\"Conversation history cleared.\")\n","\n","\n","    # --- THE UNIFIED RUN METHOD ---\n","    def run(self, user_input, task_type=\"text\", tools=None, max_retries=11, enable_retry=False,width=400,height=None):\n","        \"\"\"\n","        A single, unified method to handle text, audio, image, and tool generation.\n","        This method RETURNS data instead of displaying it.\n","        \"\"\"\n","        self._patch_retry(enable_retry)\n","\n","                # 1. Select model\n","        model_name = {\n","            \"text\": self.TEXT_MODEL,\n","            \"tool\": self.TEXT_MODEL,\n","            \"audio\": self.AUDIO_MODEL,\n","            \"image\": self.IMAGE_MODEL,\n","            \"audio_transcription\": self.TEXT_MODEL, # Use the multimodal model for transcription\n","        }.get(task_type, self.TEXT_MODEL)\n","\n","        # 2. Build messages (with a special case for transcription)\n","        if task_type == \"audio_transcription\":\n","            # For transcription, the user_input is already a fully formed HumanMessage\n","            messages = [user_input]\n","        else:\n","            messages = self._build_messages(user_input, task_type=task_type)\n","\n","        # 3. Centralized Retry Loop\n","        for attempt in range(max_retries):\n","            # api_key, user_name = get_next_key()\n","            # print(f\"➡️ Attempt {attempt + 1}/{max_retries} using key from '{user_name}'...\")\n","\n","            try:\n","                llm = ChatGoogleGenerativeAI(model=model_name, google_api_key=api_key)\n","\n","                # 4. Handle different task types\n","                if task_type == \"audio\":\n","                    tts_message = [HumanMessage(content=user_input)] # Use only the direct input for TTS\n","                    response = llm.invoke(tts_message, generation_config={\"response_modalities\": [\"AUDIO\"]})\n","                    audio_bytes = response.additional_kwargs.get(\"audio\")\n","                    self._update_history(user_input, \"[Generated audio in response to prompt]\")\n","                    return {\"type\": \"audio\", \"data\": audio_bytes, \"text\": response.content}\n","\n","                elif task_type == \"image\":\n","                    # For images, we just need the user prompt\n","                    response = llm.invoke(messages, generation_config={\"response_modalities\": [\"TEXT\",\"IMAGE\"]})\n","                    image_base64 = self._get_image_base64(response)\n","                    if image_base64:\n","                        display(Image(data=base64.b64decode(image_base64), width=width, height=height))\n","                        # Save only prompt for reference\n","                        self._update_history(user_input, \"[Generated image in response to prompt]\")\n","                        print( \"image_generated\")\n","                        return {\"type\": \"image\", \"data\": image_base64, \"text\": \"Image generated successfully.\"}\n","                    else:\n","                        print(\"No image returned\")\n","\n","                else: # Handles \"text\" and \"tool\" and \"audio_transcription\"\n","                    model_to_invoke = llm\n","                    if tools:\n","                        model_to_invoke = llm.bind_tools(tools, tool_choice=\"any\")\n","\n","                    response = model_to_invoke.invoke(messages)\n","\n","                    # Tool Execution Logic\n","                    if response.tool_calls:\n","                        messages.append(response)\n","                        for tool_call in response.tool_calls:\n","                            tool_name = tool_call[\"name\"]\n","                            tool_args = tool_call[\"args\"]\n","                            # Find the callable tool function\n","                            matched_tool_func = next((t for t in tools if getattr(t, '__name__', None) == tool_name), None)\n","                            if matched_tool_func:\n","                                result = matched_tool_func(**tool_args)\n","                                print(f\"✅ [TOOL] Called '{tool_name}' with {tool_args}. Result: {result}\")\n","                            else:\n","                                result = f\"Error: Tool '{tool_name}' not found.\"\n","                                print(f\"❌ [TOOL] {result}\")\n","                            messages.append(ToolMessage(content=str(result), tool_call_id=tool_call[\"id\"]))\n","\n","                        # Call the model again with the tool results\n","                        response = llm.invoke(messages)\n","                     # Update history differently for transcription vs. other tasks\n","                    if task_type == \"audio_transcription\":\n","                        # History entry for the audio file itself\n","                        prompt_summary = f\"[Transcription requested for an audio file]\"\n","                        self._update_history(prompt_summary, response.content)\n","                    else:\n","                        self._update_history(user_input, response.content)\n","\n","\n","                    return {\"type\": \"text\", \"data\": response.content, \"text\": response.content}\n","\n","            except (ResourceExhausted, GoogleAPICallError, ValueError) as e:\n","                print(f\"⚠️ Attempt {attempt + 1} failed: {e.__class__.__name__} - {e}\")\n","                if attempt + 1 >= max_retries:\n","                    raise RuntimeError(\"All API keys failed or quota exceeded.\") from e\n","                continue # Try the next key\n","\n","        raise RuntimeError(\"All API keys failed or quota exceeded after all attempts.\")\n","\n","        # --- NEW CONVENIENCE METHOD FOR AUDIO ---\n","    def generate_audio(self, user_input, play_audio=True):\n","        \"\"\"\n","        A convenience method that generates audio and optionally plays it.\n","        This method acts as a wrapper around the main `run()` method.\n","        \"\"\"\n","        print(f\"--- Generating Audio for: '{user_input}' ---\")\n","        response_dict = self.run(user_input, task_type=\"audio\")\n","\n","        if play_audio and response_dict and response_dict.get('type') == 'audio' and response_dict.get('data'):\n","            print(\"Audio generation successful. Playing audio...\")\n","            try:\n","                audio_bytes = response_dict['data']\n","                audio_segment = AudioSegment.from_file(BytesIO(audio_bytes), format=\"wav\")\n","                play(audio_segment)\n","                print(\"Playback finished.\")\n","            except Exception as e:\n","                print(f\"❌ Error playing audio: {e}\")\n","        elif not response_dict or not response_dict.get('data'):\n","            print(\"❌ Audio generation failed or no audio data was returned.\")\n","            return None\n","\n","        return response_dict\n","\n","        # --- NEW: GENERATE TEXT FROM AUDIO METHOD ---\n","    def generate_text_from_audio(self, file_name, enable_retry=False):\n","        \"\"\"\n","        Transcribes an audio file into text using a multimodal model.\n","\n","        Args:\n","            file_name (str): The path to the audio file (e.g., .mp3, .wav, .flac).\n","            enable_retry (bool): Whether to enable the built-in LangChain retry mechanism.\n","\n","        Returns:\n","            str: The transcribed text from the audio, or None if an error occurred.\n","        \"\"\"\n","        print(f\"--- Transcribing Audio from: '{file_name}' ---\")\n","\n","        if not os.path.exists(file_name):\n","            print(f\"❌ Error: Audio file not found at '{file_name}'\")\n","            return None\n","\n","        # 1. Automatically detect MIME type\n","        mime_type, _ = mimetypes.guess_type(file_name)\n","        if not mime_type or not mime_type.startswith(\"audio\"):\n","            print(f\"❌ Error: Could not determine a valid audio MIME type for '{file_name}'\")\n","            return None\n","\n","        print(f\"Detected MIME type: {mime_type}\")\n","\n","        # 2. Read file and encode in base64\n","        with open(file_name, \"rb\") as audio_file:\n","            encoded_audio = base64.b64encode(audio_file.read()).decode(\"utf-8\")\n","\n","        # 3. Construct the special multimodal message\n","        message = HumanMessage(\n","            content=[\n","                {\n","                    \"type\": \"text\",\n","                    \"text\": \"Transcribe the audio and provide the full text.\",\n","                },\n","                {\n","                    \"type\": \"media\",\n","                    \"data\": encoded_audio,\n","                    \"mime_type\": mime_type,\n","                },\n","            ]\n","        )\n","\n","        # 4. Call the main run method with the new task type\n","        response_dict = self.run(\n","            user_input=message,\n","            task_type=\"audio_transcription\",\n","            enable_retry=enable_retry\n","        )\n","\n","        if response_dict and response_dict.get('type') == 'text':\n","            return response_dict['text']\n","        else:\n","            print(\"❌ Audio transcription failed or no text was returned.\")\n","            return None\n"],"metadata":{"cellView":"form","id":"TlElKJTtI64I","executionInfo":{"status":"ok","timestamp":1758457635658,"user_tz":-345,"elapsed":16,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Now, use the handler to transcribe it\n","handler = LLMHandler()"],"metadata":{"id":"ZadFBNBqQFYJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now, use the handler to transcribe it\n","\n","# Transcribe the audio file\n","transcribed_text = handler.generate_text_from_audio(audio_filename)\n","\n","if transcribed_text:\n","    print(\"\\n--- Transcription Result ---\")\n","    print(transcribed_text)\n","\n","        # Define the prompt using LangChain message objects\n","    system_message = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n","\n","    user_prompt = f\"Below is an extract transcript of a Denver council meeting. Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\\n{transcription}\"\n","\n","    texthandler = LLMHandler(system_message=system_message)\n","\n","    # You can now use the transcribed text in a follow-up conversation\n","    print(\"\\n--- Minutes of the Meetings are  ---\")\n","    texthandler = LLMHandler(system_message=system_message)\n","\n","    response = texthandler.run(user_prompt)\n","    print(response['text'])\n","\n","# You can check the conversation history\n","# print(handler.conversation_history)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WsDiXiGAI7oO","executionInfo":{"status":"ok","timestamp":1758458184710,"user_tz":-345,"elapsed":28310,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}},"outputId":"2e289894-ddbf-483f-d5ec-448d75f592e6"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Transcribing Audio from: '/content/drive/MyDrive/Colab Notebooks/denver_extract.mp3' ---\n","Detected MIME type: audio/mpeg\n","\n","--- Transcription Result ---\n","and kind of the confluence of this whole idea of confluence week, the merging of two rivers. And, uh, as we've kind of seen recently in politics and in the world, there's a lot of, um, situations where water is very important right now, and it's a very big issue. So that is the reason that the back of the logo is considered water. So let you see the creation of the logo here. And yeah. So that basically, um, kind of sums up the reason behind the logo and the, um, all the meanings behind the symbolism. And, uh, I'll, you'll hear a little bit more about our Confluence Week is basically highlighting all of these indigenous events and things that are happening around Denver so that we can kind of bring more people together and kind of share this whole idea of Indigenous People's Day. So, thank you. Thank you so much, and thanks for your leadership. All right. Welcome to the Denver City Council meeting of Monday, October 9th. Please rise with the Pledge of Allegiance by Councilman Lopez. I pledge allegiance to the flag of the United States of America and to the republic for which it stands, one nation under God, indivisible, with liberty and justice for all. All right, thank you, Councilman Lopez. Madam Secretary, roll call. Black? Clark. Here. Espinosa. Here. Flynn. Gilmore? Here. Here. Cashman? Here. Kenech? Here. Lopez? Here. New? Here. Ortega? Here. Sesman? Here. Mr. President, Here. 11 present. 11 members present. We do have a quorum. Approval of the minutes. Are there any corrections to the minutes of October 2nd? Seeing none, minutes of October 2nd stand approved. Council announcements. Are there any announcements by members of council? Councilman Clark. Thank you, Mr. President. Um, I just wanted to invite everyone down to the first ever Halloween parade on Broadway in Lucky District 7. It will happen on Saturday, October 21st at 6:00 PM. It will move, um, along Broadway from 3rd to Alameda. It's going to be a fun, family friendly event. Everyone's invited to come down, wear a costume. Um, there will be candy for the kids. And, um, there are tiki zombies and, uh, 29 hearses and all kinds of fun and funky stuff on, uh, the fun and funky part of Broadway. So please join us October 21st at 6:00 for the Broadway Halloween parade. Thank you, Mr. President. All right. Thank you, Councilman Clark. I will be there. All right. Uh, presentations. Madam Secretary, do we have any presentations? None, Mr. President. Communications. Do we have any communications? None, Mr. President. We do have one proclamation this evening. Uh, proclamation 1127 in observance of, uh, the annual Indigenous People's, uh, Day in the City and County of Denver. Councilman Lopez, would you please read it? Thank you, Mr. President. With pride. Uh, proclamation number 17, well, let me just say this differently. Proclamation number 1127, series of 2017, an observance of the second annual Indigenous People's Day in the City and County of Denver. Whereas the Council of the City and County of Denver recognizes that the indigenous peoples have lived and flourished on the lands known as the Americas since time immemorial, and that Denver and the surrounding communities are built upon the ancestral homelands of numerous indigenous tribes, which include the Southern Ute, the Ute Mountain, Ute tribes of Colorado. And whereas the tribal homelands and seasonal encampments of the Arapaho and Cheyenne people along the banks of the Cherry Creek and South Platte River confluence gave bearing to the future settlements that would become the birthplace of the Mile High City. And whereas Colorado encompasses the ancestral homelands of 48 tribes, and the City and County of Denver and surrounding communities are home to the descendants of approximately 100 tribal nations. And whereas on October 3rd, 2016, the City and County of Denver unanimously passed Council Bill 801, series of 2016, officially designating the second Monday of October of each year as Indigenous People's Day in Denver, Colorado. And whereas the Council of the City and County of Denver continues to recognize and value the vast contributions made to community, made to the community through indigenous people's knowledge, science, philosophy, art, and culture. And through these contributions, the city of Denver has developed and thrived. Whereas the indigenous community, especially youth, have made great efforts this year to draw attention to the contributions of indigenous people, including Confluence Week, drawing record attendance to a national indigenous youth leadership conference, leading conversations on inclusion with their peers, and supporting increased indigenous youth participation in science and engineering. Now, therefore, be it proclaimed by the Council of the City and County of Denver, section one, that the Council of the City and County of Denver celebrates and honors the cultural and foundational contributions of indigenous people to our history, our past, our present, and future, and continues to promote the education of the Denver community of the Denver community about these historical and contemporary contributions of indigenous people. Section two, that the City and County of Denver, Colorado, does hereby observe October 9th, 2017 as Indigenous People's Day. Section three, that the clerk of the City and County of Denver shall attest and affix the seal of the City and County of Denver to this proclamation, and that a copy be transmitted transmitted, excuse me, to the Denver American Indian Commission, the City and County of Denver School District Number One, and the Colorado Commission on Indian Affairs. Thank you, Councilman Lopez. Your motion to adopt. Mr. President, I move that proclamation number 1127, series of 2017 be adopted. All right. It has been moved and seconded. Council members of council, Councilman Lopez. Thank you, Mr. President. Um, it gives me a lot of pleasure and, and, and pride to read this proclamation officially for this, for the third time, but as Indigenous People's Day in Denver officially for the second time. Um, it is, uh, it's always awesome to be able to see, um, not just this proclamation come through, come by my desk, but to see so many different people from our community in our, in our council chambers. Um, it was a very beautiful piece of artwork that you presented to us earlier, and it is exactly the spirit that we, um, drafted this proclamation and this actual, the, the ordinance that created Indigenous People's Day, when we sat down and wrote it and as a community, we couldn't think of anything else to begin, except for the confluence of the two rivers. And those confluence of the two rivers created such a great city. And we live in such an amazing city, and we we're all proud of it, and sometimes we, um, and, and a lot of people from all over the country or all over the world are proud of it, and sometimes a little too proud of it. I just tell them to go back home. Um, but, um, I'm, I'm kidding when I say that. Um, but the, the, the really nice thing about this is that, um, we are celebrating Indigenous People's Day out of pride for who we are, who we are as a city and the, and the contributions of indigenous people to the city, not out of spite, not out of, um, a replacement of one culture over the other or, or out of, um, contempt or, um, or disrespect. You know, I, I think of, uh, a quote that Cesar Chavez, uh, made very, um, very popular. And, uh, it's stuck with me for a very long time and, and, and I, any of, any time I have the opportunity, I, to speak in front of children and especially children in our community that, you know, they, they, um, often second guess themselves and where they're coming from, who they are. And, um, and I always say that, you know, it's, it's very important to be proud of where we are from. And the quote that I use from Cesar Chavez is, um, you know, pride in one's own culture does not require contempt or disrespect of another, right? And that's very important. It's very important for us to recognize that. No matter who we are, where we come from in this society, that your pride in your own culture doesn't require, should not require the contempt or disrespect of another. And man, what a year to be for that to just sit on our shoulders for a while, for us to think about, right? And so I wanted to, um, uh, just to, to thank you all, to thank the commission. Um, there's going to be a couple individuals that are going to come and speak. Thank you for your art, your lovely artwork for us to see, uh, what's in your heart and what now has become, it probably's going to be a very important symbol, uh, for the community. And also just for the work, the, the daily work, every single day. We still have a lot of brothers and sisters whose ancestors once lived on these lands freely now stand on street corners, right, in poverty. Um, without access to services, right? Um, without access to sobriety or even housing or jobs. And what a, what a what a cruel way to pay back a culture that has paved the way for the city to be built upon its shores, right? So, um, we have a lot of work to do. And these kind of proclamations and this day is not a day off, it's a day on in Denver, right? And, and, and addressing those, those, those critical issues. So, um, I, I know that my colleagues are, uh, very supportive. I'm going to ask you to support this proclamation as I know you always have done in the past. I'm very proud of today. Oh, and we made Time Magazine and Newsweek once again today as being a leader in terms of the cities that are celebrating Indigenous People's Day. I wanted to make a point out of that. Thank you, uh, Councilman Lopez, and thank you for sponsoring this. Councilwoman Ortega? Mr. President, I want to ask that my name be added. I don't think I could, um, add much more to what Councilman Lopez has, um, shared with us. Um, I want to thank him for bringing this forward and, um, really just appreciate all the contributions that our, um, Native American community has contributed to this great city and great state. Um, I worked in the Lieutenant Governor's office when the Commission on Indian Affairs was created and had the benefit of being able to go down to, um, the four corners for a peace treaty signing ceremony between the Utes and the Comanches that had been sort of at, at odds with each other for about 100 years. And just being able to participate in that powwow was, was pretty awesome. So, um, and for those of you who continue to participate in the annual powwow, it's, it's such a great opportunity for everybody else to enjoy so many of the contributions of the culture. I mean, to see that the dance continues to be carried on, as well as, as the native language from generation to generation is just so incredible because in so many cultures, you know, people have come here and, um, assimilated to the, the, you know, the norms here and they lose their language and, and, and lose a lot of the culture. And in the Native community, that, that hasn't happened. That has that, you know, um, commitment to just passing that on from generation to generation is, is so important. And so I'm, I'm happy to be a co-sponsor of this tonight. Thank you. All right, thank you, Councilwoman Ortega. Councilwoman Kenech. Thank you very much, and I also want to thank my colleague for bringing this forward. Um, and I, I, I just wanted to to, to say a word to the artist about how beautiful and moving I thought this logo was and your description of it. Um, and I think one of the things, um, that is clear is, you know, the words sometimes don't convey the power of imagery or music or the other pieces that make up culture. And so I think the art is so important. And when you talked about water, I was also thinking about land, and I guess I just wanted to say thank you. Um, many of the Native American peoples of Colorado have been at the forefront, or actually nationally, of defending some of the the, um, public lands that have been protected over the last few years that are under attack right now. And they're places that you all, um, the communities have fought to protect, but that everyone gets to enjoy. And so I just think that it's an example of where cultural preservation intersects with environmental protection, with, you know, recreation, and all of the other ways that that public lands are so important. And so I think, um, I just wanted to say thank you for that because I think we have some very sacred places in our country that are at risk right now. And, um, so as we celebrate, I appreciate that there's still a piece of resistance in here, and I think that, um, I just want to mention a solidarity in, in, in mention a feeling of solidarity with that resistance. So, thank you, and, uh, happy Confluence Week. Thank you, uh, Councilwoman Kenech. Uh, and seeing no other comments, I'll just say a couple. And, um, in a time of such divisive, um, ugliness and just despicable behavior from our leadership, um, the reason I'm so supportive of Indigenous People's Day is because it means inclusivity. It means respecting all, respecting those who have been silenced, um, uh, on purpose for a long time and whose history has not been told. And so we celebrate inclusivity in the face of such, um, such evil times, honestly.\n","\n","--- Minutes of the Meetings are  ---\n","## Denver City Council Meeting Minutes - October 9, 2017\n","\n","**Summary:**\n","\n","The Denver City Council convened on October 9, 2017, for a regular meeting. Key items included the adoption of Proclamation 1127, observing Indigenous People's Day in the City and County of Denver. Council members discussed the importance of recognizing the contributions of indigenous peoples, celebrating inclusivity, and addressing ongoing challenges faced by the Native American community. The meeting included announcements and presentations.\n","\n","**Attendees:**\n","\n","*   Councilman Black\n","*   Councilman Clark\n","*   Councilman Espinosa\n","*   Councilman Flynn\n","*   Councilman Gilmore\n","*   Councilman Cashman\n","*   Councilman Kenech\n","*   Councilman Lopez\n","*   Councilman New\n","*   Councilwoman Ortega\n","*   Councilman Susman\n","*   Mr. President\n","\n","**Location:** Denver City Council Chambers\n","\n","**Date:** October 9, 2017\n","\n","### Discussion Points\n","\n","*   **Indigenous People's Day Proclamation:**\n","    *   Councilman Lopez read Proclamation 1127, series of 2017, which officially designates the second Monday of October as Indigenous People's Day in Denver.\n","    *   The proclamation recognizes the historical and contemporary contributions of indigenous people to the city.\n","    *   Council members emphasized the importance of celebrating indigenous culture out of pride and respect, not out of spite or contempt for other cultures.\n","    *   Councilman Lopez highlighted the need to address the challenges faced by Native Americans in Denver, including poverty, lack of access to services, and homelessness.\n","    *   Councilwoman Ortega emphasized the importance of cultural preservation and language maintenance within the Native American community.\n","    *   Councilwoman Kenech expressed appreciation for the role of Native American peoples in defending public lands and highlighted the intersection of cultural preservation and environmental protection.\n","    *   Mr. President emphasized that Indigenous People's Day promotes inclusivity and respects those whose history has been silenced.\n","*   **Confluence Week:**\n","    *   Confluence week is highlighting all of these indigenous events and things that are happening around Denver so that we can kind of bring more people together and kind of share this whole idea of indigenous people's day.\n","*   **Halloween Parade Announcement:**\n","    *   Councilman Clark announced the first-ever Halloween parade on Broadway in Lucky District 7, scheduled for October 21st at 6:00 p.m.\n","\n","### Takeaways\n","\n","*   Denver City Council is committed to recognizing and honoring the contributions of indigenous people to the city's history, culture, and future.\n","*   Indigenous People's Day is an opportunity to celebrate inclusivity, promote education, and address the challenges faced by the Native American community.\n","*   Cultural preservation, language maintenance, and environmental protection are important aspects of supporting indigenous communities.\n","*   The Council acknowledged the importance of art and symbolism in conveying the power of culture.\n","\n","### Action Items\n","\n","*   **Council Members:** Support the initiatives and programs aimed at addressing the needs of the Native American community in Denver. (Ongoing)\n","    *   Owner: All Council Members\n","*   **Council Members:** Attend and promote the Broadway Halloween Parade on October 21st.\n","    *   Owner: Councilman Clark, All Council Members\n","*   **Clerk of the City and County of Denver:** Transmit a copy of Proclamation 1127 to the Denver American Indian Commission, the City and County of Denver School District number one, and the Colorado Commission on Indian Affairs. (Completed)\n","    *   Owner: Clerk of the City and County of Denver\n"]}]},{"cell_type":"code","source":["display(Markdown(response['text']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ncl_U0W2Rtsk","executionInfo":{"status":"ok","timestamp":1758458220240,"user_tz":-345,"elapsed":35,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}},"outputId":"13631744-5d19-41f5-8203-87f443632143"},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## Denver City Council Meeting Minutes - October 9, 2017\n\n**Summary:**\n\nThe Denver City Council convened on October 9, 2017, for a regular meeting. Key items included the adoption of Proclamation 1127, observing Indigenous People's Day in the City and County of Denver. Council members discussed the importance of recognizing the contributions of indigenous peoples, celebrating inclusivity, and addressing ongoing challenges faced by the Native American community. The meeting included announcements and presentations.\n\n**Attendees:**\n\n*   Councilman Black\n*   Councilman Clark\n*   Councilman Espinosa\n*   Councilman Flynn\n*   Councilman Gilmore\n*   Councilman Cashman\n*   Councilman Kenech\n*   Councilman Lopez\n*   Councilman New\n*   Councilwoman Ortega\n*   Councilman Susman\n*   Mr. President\n\n**Location:** Denver City Council Chambers\n\n**Date:** October 9, 2017\n\n### Discussion Points\n\n*   **Indigenous People's Day Proclamation:**\n    *   Councilman Lopez read Proclamation 1127, series of 2017, which officially designates the second Monday of October as Indigenous People's Day in Denver.\n    *   The proclamation recognizes the historical and contemporary contributions of indigenous people to the city.\n    *   Council members emphasized the importance of celebrating indigenous culture out of pride and respect, not out of spite or contempt for other cultures.\n    *   Councilman Lopez highlighted the need to address the challenges faced by Native Americans in Denver, including poverty, lack of access to services, and homelessness.\n    *   Councilwoman Ortega emphasized the importance of cultural preservation and language maintenance within the Native American community.\n    *   Councilwoman Kenech expressed appreciation for the role of Native American peoples in defending public lands and highlighted the intersection of cultural preservation and environmental protection.\n    *   Mr. President emphasized that Indigenous People's Day promotes inclusivity and respects those whose history has been silenced.\n*   **Confluence Week:**\n    *   Confluence week is highlighting all of these indigenous events and things that are happening around Denver so that we can kind of bring more people together and kind of share this whole idea of indigenous people's day.\n*   **Halloween Parade Announcement:**\n    *   Councilman Clark announced the first-ever Halloween parade on Broadway in Lucky District 7, scheduled for October 21st at 6:00 p.m.\n\n### Takeaways\n\n*   Denver City Council is committed to recognizing and honoring the contributions of indigenous people to the city's history, culture, and future.\n*   Indigenous People's Day is an opportunity to celebrate inclusivity, promote education, and address the challenges faced by the Native American community.\n*   Cultural preservation, language maintenance, and environmental protection are important aspects of supporting indigenous communities.\n*   The Council acknowledged the importance of art and symbolism in conveying the power of culture.\n\n### Action Items\n\n*   **Council Members:** Support the initiatives and programs aimed at addressing the needs of the Native American community in Denver. (Ongoing)\n    *   Owner: All Council Members\n*   **Council Members:** Attend and promote the Broadway Halloween Parade on October 21st.\n    *   Owner: Councilman Clark, All Council Members\n*   **Clerk of the City and County of Denver:** Transmit a copy of Proclamation 1127 to the Denver American Indian Commission, the City and County of Denver School District number one, and the Colorado Commission on Indian Affairs. (Completed)\n    *   Owner: Clerk of the City and County of Denver"},"metadata":{}}]},{"cell_type":"code","source":["audio_filename"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"cnzT-pTZO5i3","executionInfo":{"status":"ok","timestamp":1758457646945,"user_tz":-345,"elapsed":18,"user":{"displayName":"Sijan Paudel","userId":"05175123034513638283"}},"outputId":"751c6d6f-dcdf-4f2e-d1c8-b72b0dfb3bc7"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Colab Notebooks/denver_extract.mp3'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":[],"metadata":{"id":"s9aYB6-qO4Wc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","### Updated LangChain Code\n","# 1. INSTALLS\n","# Make sure to install the updated langchain packages\n","# !pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n","# !pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0\n","# !pip install -q openai langchain langchain-huggingface\n","\n","# 2. IMPORTS\n","import os\n","import torch\n","from openai import OpenAI\n","from google.colab import drive, userdata\n","from huggingface_hub import login\n","from IPython.display import Markdown, display\n","from transformers import BitsAndBytesConfig\n","\n","# LangChain specific imports\n","from langchain_huggingface import HuggingFacePipeline\n","from langchain_core.messages import SystemMessage, HumanMessage\n","from langchain_core.output_parsers import StrOutputParser\n","\n","\n","# 3. CONSTANTS AND SETUP\n","AUDIO_MODEL = \"whisper-1\"\n","LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n","\n","\n","\n","# 4. AUTHENTICATION\n","# Sign in to HuggingFace Hub\n","hf_token = userdata.get('HF_TOKEN')\n","login(hf_token, add_to_git_credential=True)\n","\n","\n","\n","# Transcribe the audio file\n","transcription = handler.generate_text_from_audio(audio_filename)\n","print(\"Transcription complete.\")\n","print(\"-\" * 20)\n","print(transcription) # Uncomment to see the full transcript\n","\n","\n","# ==============================================================================\n","# 6. LANGCHAIN IMPLEMENTATION\n","# ==============================================================================\n","\n","# Define the prompt using LangChain message objects\n","system_message = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n","user_prompt = f\"Below is an extract transcript of a Denver council meeting. Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\\n{transcription}\"\n","\n","messages = [\n","    SystemMessage(content=system_message),\n","    HumanMessage(content=user_prompt)\n","]\n","\n","# Define the quantization configuration\n","quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    bnb_4bit_quant_type=\"nf4\"\n",")\n","\n","# Create the LangChain HuggingFacePipeline\n","# This class handles loading the model and tokenizer, and setting up the text-generation pipeline\n","llm = HuggingFacePipeline.from_model_id(\n","    model_id=LLAMA,\n","    task=\"text-generation\",\n","    model_kwargs={\"quantization_config\": quant_config},\n","    device_map=\"auto\",\n","    pipeline_kwargs={\"max_new_tokens\": 2000},\n",")\n","\n","# Create the LCEL chain by piping the components together\n","# The StrOutputParser ensures the output is a clean string\n","chain = llm | StrOutputParser()\n","\n","# Stream the response and collect the full text\n","print(\"Generating meeting minutes...\\n\")\n","full_response = \"\"\n","for chunk in chain.stream(messages):\n","    print(chunk, end=\"\", flush=True)\n","    full_response += chunk\n","\n","print(\"\\n\\n\" + \"=\"*50 + \"\\nGeneration complete.\\n\" + \"=\"*50)\n","\n","# Display the final, complete response as formatted Markdown\n","display(Markdown(full_response))\n","\n","\n","# ==============================================================================\n","# The alternative open-source transcription part can remain the same\n","# as it uses the `transformers` library directly for a specific task.\n","# =============================================================================="],"metadata":{"id":"LvPzeco_MC3Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Of course. This is an excellent example of using a cutting-edge, asynchronous API.\n","\n","Since Google's Veo video generation is a long-running, asynchronous task that requires polling, it doesn't fit the standard synchronous `invoke` or `stream` pattern of most LangChain models.\n","\n","The best and most \"LangChain-idiomatic\" way to handle this is to create a **custom LangChain Runnable**. This encapsulates the entire asynchronous workflow (initiation, polling, and result extraction) into a single, chainable component. This makes your complex workflow clean, reusable, and compatible with the LangChain Expression Language (LCEL).\n","\n","Here is the equivalent LangChain code, structured within a custom runnable class.\n","\n","### Key LangChain Concepts Used:\n","\n","*   **`langchain_core.runnables.RunnableSerializable`**: The base class for creating custom, chainable components in LCEL. Our new `GoogleVeoVideoGenerator` will inherit from this.\n","*   **`.invoke()` method**: The core method of our runnable. It will contain the logic to start the video generation and poll for its completion.\n","*   **Encapsulation**: We hide the complex `while` loop and API calls inside our runnable, exposing a simple interface that just takes a prompt and returns a result.\n","\n","---\n","\n","### LangChain Equivalent Code with Custom Runnable\n","\n","```python\n","import os\n","import time\n","from typing import Any, Dict, Optional\n","\n","# Make sure to install the necessary libraries\n","# !pip install -q langchain google-generativeai langchain-google-genai\n","\n","import google.generativeai as genai\n","from google.generativeai.client import File, Operation\n","from google.generativeai.types.generative_models import GeneratedVideo\n","from langchain_core.runnables import RunnableSerializable, RunnableConfig\n","from pydantic.v1 import Field, PrivateAttr\n","\n","# --- Step 1: Configure your API Key ---\n","# Make sure to set your Google API Key in your environment or Colab secrets\n","# from google.colab import userdata\n","# GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n","# genai.configure(api_key=GOOGLE_API_KEY)\n","\n","# A placeholder for the actual API key setup\n","if \"GOOGLE_API_KEY\" not in os.environ:\n","    print(\"⚠️ GOOGLE_API_KEY environment variable not set.\")\n","    # You would typically exit or raise an error here\n","else:\n","    genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n","\n","\n","# --- Step 2: Create a Custom LangChain Runnable for Veo ---\n","\n","class GoogleVeoVideoGenerator(RunnableSerializable):\n","    \"\"\"\n","    A custom LangChain Runnable to generate videos using Google's Veo model.\n","    \n","    This runnable encapsulates the asynchronous workflow of:\n","    1. Starting the video generation task.\n","    2. Polling the operation until the video is ready.\n","    3. Returning the final generated video object.\n","    \"\"\"\n","    model: str = \"veo-3.0-generate-001\"\n","    polling_interval_seconds: int = 10\n","    \n","    # Use PrivateAttr for the client to avoid serialization issues\n","    _client: Any = PrivateAttr()\n","\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","        self._client = genai.Client()\n","\n","    def invoke(self, input: Dict, config: Optional[RunnableConfig] = None) -> Optional[GeneratedVideo]:\n","        \"\"\"\n","        Takes a dictionary with a 'prompt' key and returns a GeneratedVideo object.\n","        \"\"\"\n","        prompt = input.get(\"prompt\")\n","        if not prompt:\n","            raise ValueError(\"Input dictionary must contain a 'prompt' key.\")\n","\n","        print(f\"▶️ Starting video generation for prompt: '{prompt[:50]}...'\")\n","        \n","        # 1. Start the asynchronous generation operation\n","        operation = self._client.models.generate_videos(\n","            model=self.model,\n","            prompt=prompt,\n","        )\n","\n","        print(f\"⏳ Operation initiated. Polling every {self.polling_interval_seconds} seconds...\")\n","\n","        # 2. Poll the operation status until the video is ready\n","        while not operation.done:\n","            print(\"   - Waiting for video to complete...\")\n","            time.sleep(self.polling_interval_seconds)\n","            operation = self._client.operations.get(operation.name)\n","\n","        # 3. Check for errors and get the result\n","        if operation.error:\n","            print(f\"❌ Video generation failed with error: {operation.error.message}\")\n","            return None\n","            \n","        if not operation.response or not operation.response.generated_videos:\n","            print(\"❌ Operation completed but no video was returned.\")\n","            return None\n","\n","        print(\"✅ Video generation complete!\")\n","        \n","        # 4. Return the generated video object\n","        return operation.response.generated_videos[0]\n","\n","\n","# --- Step 3: Create a Helper Function to Save the Video ---\n","# This separates the generation logic from the saving logic.\n","\n","def save_video_from_response(video_object: GeneratedVideo, output_filename: str):\n","    \"\"\"Saves the generated video to a local file.\"\"\"\n","    if not video_object:\n","        print(\"Cannot save video, the generation result is empty.\")\n","        return\n","        \n","    print(f\"💾 Downloading and saving generated video to '{output_filename}'...\")\n","    try:\n","        # The video object has a 'video' attribute which is a File object\n","        video_file = video_object.video\n","        video_file.save(output_filename)\n","        print(f\"✅ Video successfully saved to '{output_filename}'\")\n","    except Exception as e:\n","        print(f\"❌ An error occurred while saving the video: {e}\")\n","\n","\n","# --- Step 4: Execute the Workflow ---\n","\n","# The prompt from your example\n","prompt = \"\"\"A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.\n","A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'\"\"\"\n","\n","# Instantiate our custom LangChain runnable\n","video_generator_chain = GoogleVeoVideoGenerator()\n","\n","# Invoke the chain. This single call will handle the start, polling, and completion.\n","# The input is a dictionary, which is standard for LCEL.\n","generated_video_result = video_generator_chain.invoke({\"prompt\": prompt})\n","\n","# Use our helper to save the result\n","if generated_video_result:\n","    save_video_from_response(generated_video_result, \"dialogue_example_langchain.mp4\")\n","\n","```\n","\n","### How This LangChain Version Improves on the Original:\n","\n","1.  **Reusability**: The `GoogleVeoVideoGenerator` is now a reusable component. You can plug it into any LangChain chain with the `|` operator, just like any other LLM.\n","2.  **Abstraction**: It hides the messy details of the `while` loop. The end-user only needs to call `.invoke()` with a prompt, making the main script much cleaner and easier to read.\n","3.  **LCEL Compatibility**: As a `Runnable`, you can now chain it with other components. For example, you could have a `ChatGoogleGenerativeAI` model generate a creative prompt first, and then pipe its output directly into `video_generator_chain`.\n","4.  **Separation of Concerns**: The logic to *generate* the video is cleanly separated from the logic to *save* the video, which is a better software design practice."],"metadata":{"id":"ylT3Zv65OGM9"}}]}