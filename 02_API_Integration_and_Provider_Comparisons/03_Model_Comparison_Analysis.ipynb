{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
   "metadata": {},
   "source": [
    "# Day 3 - Conversational AI - aka Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "231605aa-fccb-447e-89cf-8b187444536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key not set\n",
      "Anthropic API Key not set\n",
      "Google API Key not set\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6541d58e-2297-4de1-b1f7-77da1b98b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "\n",
    "# openai = OpenAI()\n",
    "# MODEL = 'gpt-4o-mini'\n",
    "MODEL = \"gemini-2.0-flash\"\n",
    "model = ChatGoogleGenerativeAI(model=MODEL, api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e16839b5-c03b-4d9d-add6-87a0f6f37575",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e97227-f162-4d1a-a0b2-345ff248cbe7",
   "metadata": {},
   "source": [
    "# Please read this! A change from the video:\n",
    "\n",
    "In the video, I explain how we now need to write a function called:\n",
    "\n",
    "`chat(message, history)`\n",
    "\n",
    "Which expects to receive `history` in a particular format, which we need to map to the OpenAI format before we call OpenAI:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "But Gradio has been upgraded! Now it will pass in `history` in the exact OpenAI format, perfect for us to send straight to OpenAI.\n",
    "\n",
    "So our work just got easier!\n",
    "\n",
    "We will write a function `chat(message, history)` where:  \n",
    "**message** is the prompt to use  \n",
    "**history** is the past conversation, in OpenAI format  \n",
    "\n",
    "We will combine the system message, history and latest message, then call OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler than in my video - we can easily create this function that calls OpenAI\n",
    "# It's now just 1 line of code to prepare the input to OpenAI!\n",
    "\n",
    "# Student Octavio O. has pointed out that this isn't quite as straightforward for Claude -\n",
    "# see the excellent contribution in community-contributions \"Gradio_issue_with_Claude\" that handles Claude.\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    print(\"History is:\")\n",
    "    print(history)\n",
    "    print(\"And messages is:\")\n",
    "    print(messages)\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3e969-b492-47b2-8b36-c3160e0c06d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52354acb-109f-40df-9b72-fcf30cc3fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []  \n",
    "def stream_chat(user_input, model_name):\n",
    "    global conversation_history\n",
    "\n",
    "    # Convert previous history to LangChain messages\n",
    "    lc_history = []\n",
    "    for h in conversation_history:\n",
    "        role = h[\"role\"]\n",
    "        content = h[\"content\"]\n",
    "        if role == \"user\":\n",
    "            lc_history.append(HumanMessage(content=content))\n",
    "        elif role == \"assistant\":\n",
    "            lc_history.append(AIMessage(content=content))\n",
    "        else:\n",
    "            lc_history.append(SystemMessage(content=content))\n",
    "\n",
    "    # Full messages: system + history + current user\n",
    "    messages = [SystemMessage(content=system_message)] + lc_history + [HumanMessage(content=user_input)]\n",
    "    print(f\"{messages}\\n\\n\\n\")\n",
    "\n",
    "    # Stream the response from the model\n",
    "    response_text = \"\"\n",
    "    for chunk in model.stream(messages):\n",
    "        if chunk.content:\n",
    "            response_text += chunk.content\n",
    "            yield response_text\n",
    "\n",
    "    # Save to conversation history for next prompt\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "    print(f\"{conversation_history}\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334422a-808f-4147-9c4c-57d63d9780d0",
   "metadata": {},
   "source": [
    "## And then enter Gradio's magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0866ca56-100a-44ab-8bd0-1568feaf6bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7907\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7907/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=stream_chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f91b414-8bab-472d-b9c9-3fa51259bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'\\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4e5be3ec-c26c-42bc-ac16-c39d369883f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []  \n",
    "def stream_chat(user_input,model_name):\n",
    "    global conversation_history\n",
    "\n",
    "    # Convert previous history to LangChain messages\n",
    "    lc_history = []\n",
    "    for h in conversation_history:\n",
    "        role = h[\"role\"]\n",
    "        content = h[\"content\"]\n",
    "        if role == \"user\":\n",
    "            lc_history.append(HumanMessage(content=content))\n",
    "        elif role == \"assistant\":\n",
    "            lc_history.append(AIMessage(content=content))\n",
    "        else:\n",
    "            lc_history.append(SystemMessage(content=content))\n",
    "\n",
    "    # Full messages: system + history + current user\n",
    "    messages = [SystemMessage(content=system_message)] + lc_history + [HumanMessage(content=user_input)]\n",
    "    print(f\"{messages}\\n\\n\\n\")\n",
    "\n",
    "    # Stream the response from the model\n",
    "    response_text = \"\"\n",
    "    for chunk in model.stream(messages):\n",
    "        if chunk.content:\n",
    "            response_text += chunk.content\n",
    "            yield response_text\n",
    "\n",
    "    # Save to conversation history for next prompt\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "    print(f\"{conversation_history}\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "413e9e4e-7836-43ac-a0c3-e1ab5ed6b136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7908\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7908/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"You are a helpful assistant in a clothes store. You should try to gently encourage the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. For example, if the customer says 'I'm looking to buy a hat', you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'Encourage the customer to buy hats if they are unsure what to get.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='hi', additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "\n",
      "\n",
      "[{'role': 'user', 'content': 'hi'}, {'role': 'assistant', 'content': 'Hi there! Welcome to our store. Are you looking for anything in particular today? We have a great sales event going on right now, with most items 50% off! And all our hats are an amazing 60% off!\\n'}]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=stream_chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d75f0ffa-55c8-4152-b451-945021676837",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
    "but remind the customer to look at hats!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7dfbc15d-e230-4821-b40f-639facfd8a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_google_genai.chat_models.ChatGoogleGenerativeAI'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c602a8dd-2df7-4eb7-b539-4e01865a6351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7910\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7910/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"You are a helpful assistant in a clothes store. You should try to gently encourage the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. For example, if the customer says 'I'm looking to buy a hat', you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'Encourage the customer to buy hats if they are unsure what to get.\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, but remind the customer to look at hats!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content='Hi there! Welcome to our store. Are you looking for anything in particular today? We have a great sales event going on right now, with most items 50% off! And all our hats are an amazing 60% off!\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi', additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "\n",
      "\n",
      "[{'role': 'user', 'content': 'hi'}, {'role': 'assistant', 'content': 'Hi there! Welcome to our store. Are you looking for anything in particular today? We have a great sales event going on right now, with most items 50% off! And all our hats are an amazing 60% off!\\n'}, {'role': 'user', 'content': 'hi'}, {'role': 'assistant', 'content': 'Welcome! Is there something I can help you find?\\n'}]\n",
      "\n",
      "\n",
      "\n",
      "[SystemMessage(content=\"You are a helpful assistant in a clothes store. You should try to gently encourage the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. For example, if the customer says 'I'm looking to buy a hat', you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'Encourage the customer to buy hats if they are unsure what to get.\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, but remind the customer to look at hats!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content='Hi there! Welcome to our store. Are you looking for anything in particular today? We have a great sales event going on right now, with most items 50% off! And all our hats are an amazing 60% off!\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content='Welcome! Is there something I can help you find?\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='I want shoes', additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "\n",
      "\n",
      "[{'role': 'user', 'content': 'hi'}, {'role': 'assistant', 'content': 'Hi there! Welcome to our store. Are you looking for anything in particular today? We have a great sales event going on right now, with most items 50% off! And all our hats are an amazing 60% off!\\n'}, {'role': 'user', 'content': 'hi'}, {'role': 'assistant', 'content': 'Welcome! Is there something I can help you find?\\n'}, {'role': 'user', 'content': 'I want shoes'}, {'role': 'assistant', 'content': \"Okay, we have a great selection of shoes! Just so you know, shoes aren't included in our sales event today, but be sure to check out our hat selection while you're here - they are 60% off!\\n\"}]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=stream_chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0516d2a-9713-4306-a824-45dbed435cd2",
   "metadata": {},
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a987a66-1061-46d6-a83a-a30859dc88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed a bug in this function brilliantly identified by student Gabor M.!\n",
    "# I've also improved the structure of this function\n",
    "\n",
    "def chat(message, history):\n",
    "\n",
    "    relevant_system_message = system_message\n",
    "    if 'belt' in message:\n",
    "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bf540a8a-060d-467e-bd68-e1a1d3836816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_google_genai.chat_models.ChatGoogleGenerativeAI'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8db93577-a87b-4e88-b56f-1235dbb3e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "def stream_chat(user_input,m):\n",
    "    global conversation_history\n",
    "\n",
    "    # Update system message if 'belt' is mentioned\n",
    "    relevant_system_message = system_message\n",
    "    if 'belt' in user_input.lower():  # lowercase to make it case-insensitive\n",
    "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
    "\n",
    "    # Convert previous history to LangChain messages\n",
    "    lc_history = []\n",
    "    for h in conversation_history:\n",
    "        role = h[\"role\"]\n",
    "        content = h[\"content\"]\n",
    "        if role == \"user\":\n",
    "            lc_history.append(HumanMessage(content=content))\n",
    "        elif role == \"assistant\":\n",
    "            lc_history.append(AIMessage(content=content))\n",
    "        else:\n",
    "            lc_history.append(SystemMessage(content=content))\n",
    "\n",
    "    # Full messages: system + history + current user\n",
    "    messages = [SystemMessage(content=relevant_system_message)] + lc_history + [HumanMessage(content=user_input)]\n",
    "    print(f\"Messages:\\n{messages}\\n\\n\")\n",
    "\n",
    "    # Stream the response from the model\n",
    "    response_text = \"\"\n",
    "    for chunk in model.stream(messages):\n",
    "        if chunk.content:\n",
    "            response_text += chunk.content\n",
    "            yield response_text\n",
    "\n",
    "    # Save to conversation history for next prompt\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "    print(f\"Conversation history:\\n{conversation_history}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "20570de2-eaad-42cc-a92c-c779d71b48b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7914\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7914/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages:\n",
      "[SystemMessage(content=\"You are a helpful assistant in a clothes store. You should try to gently encourage the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. For example, if the customer says 'I'm looking to buy a hat', you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'Encourage the customer to buy hats if they are unsure what to get.\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, but remind the customer to look at hats!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='hi', additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "\n",
      "Conversation history:\n",
      "[{'role': 'user', 'content': 'hi'}, {'role': 'assistant', 'content': \"Hello! Welcome to our store. What are you looking for today? I'd be happy to help you find something. We have a great sales event happening right now, with many items at 50% off! And don't forget to check out our hats - they're 60% off!\\n\"}]\n",
      "\n",
      "\n",
      "Messages:\n",
      "[SystemMessage(content=\"You are a helpful assistant in a clothes store. You should try to gently encourage the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. For example, if the customer says 'I'm looking to buy a hat', you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'Encourage the customer to buy hats if they are unsure what to get.\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, but remind the customer to look at hats! The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! Welcome to our store. What are you looking for today? I'd be happy to help you find something. We have a great sales event happening right now, with many items at 50% off! And don't forget to check out our hats - they're 60% off!\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='I want belts', additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "\n",
      "Conversation history:\n",
      "[{'role': 'user', 'content': 'hi'}, {'role': 'assistant', 'content': \"Hello! Welcome to our store. What are you looking for today? I'd be happy to help you find something. We have a great sales event happening right now, with many items at 50% off! And don't forget to check out our hats - they're 60% off!\\n\"}, {'role': 'user', 'content': 'I want belts'}, {'role': 'assistant', 'content': \"Ah, unfortunately, we don't carry belts here. However, since you're looking to accessorize, have you seen our selection of hats? They're 60% off today! We also have many other items on sale at 50% off if you'd like to browse.\\n\"}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=stream_chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a57ee0-b945-48a7-a024-01b56a5d4b3e",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business Applications</h2>\n",
    "            <span style=\"color:#181;\">Conversational Assistants are of course a hugely common use case for Gen AI, and the latest frontier models are remarkably good at nuanced conversation. And Gradio makes it easy to have a user interface. Another crucial skill we covered is how to use prompting to provide context, information and examples.\n",
    "<br/><br/>\n",
    "Consider how you could apply an AI Assistant to your business, and make yourself a prototype. Use the system prompt to give context on your business, and set the tone for the LLM.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb9e21-df67-4c2b-b952-5e7e7961b03d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
