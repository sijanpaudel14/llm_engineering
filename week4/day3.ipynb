{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6ab9a2-28a2-445d-8512-a0dc8d1b54e9",
   "metadata": {},
   "source": [
    "# Code Generator\n",
    "\n",
    "The requirement: use a Frontier model to generate high performance C++ code from Python code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ccb926-7b49-44a4-99ab-8ef20b5778c0",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder: fetch latest code</h2>\n",
    "            <span style=\"color:#f71;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml --prune</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e04a2-5b8a-4fd5-9db8-27c02f033313",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h1 style=\"color:#900;\">Important Note</h1>\n",
    "            <span style=\"color:#900;\">\n",
    "            In this lab, I use GPT-4o and Claude-3.5-Sonnet, which are the slightly higher priced models. The costs are still low, but if you'd prefer to keep costs ultra low, please make the suggested switches to the models (3 cells down from here).\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ffe99924-1e15-4851-abb9-eed23246176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from environment variables\n",
    "api_key = os.getenv(\"OPEN_ROUTER_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88b5a1f4-8f68-4ef3-99d5-8c2a3043c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "api = OpenRouter(api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "febe5c2f-0c17-4253-876b-a957a4cc6a84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure, I can explain the theory of relativity in simple terms. There are actually two parts to Einstein's theory of relativity:\n",
       "\n",
       "1. **Special Relativity**: This theory deals with objects moving at constant speeds, particularly those approaching the speed of light. The key ideas are:\n",
       "   - **The Speed of Light is Constant**: No matter how fast you move, lightspeed appears the same if you're measuring it.\n",
       "   - **Time Dilation**: Time moves slower for objects moving at high speeds compared to those at rest, assuming someone else in space sees them both.\n",
       "   - **Length Contraction**: Objects moving at high speeds appear shorter along the direction of motion.\n",
       "   - **Equivalence of Mass and Energy**: This is expressed by the famous equation \\(E = mc^2\\), which means energy (E) and mass (m) are interchangeable; c is the speed of light.\n",
       "\n",
       "2. **General Relativity**: This theory extends the idea of special relativity and deals with gravity:\n",
       "   - **Gravity is Curvature of Spacetime**: According to general relativity, massive objects like planets and stars bend the fabric of spacetime around them.\n",
       "   - **Everything Follows the Bend**: Other objects move along the curves in spacetime caused by massive objects, which we perceive as gravity.\n",
       "\n",
       "In essence, what these theories show us is that our everyday experiences of space, time, and gravity are more complex and interconnected than we might originally think."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(full_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "413112a9-aadf-41e7-b5e6-49530a932b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cpp=\"\"\"\n",
    "// To compile on M1 Mac with clang for maximum performance:\n",
    "// clang++ -std=c++17 -O3 -march=native -Xpreprocessor -fopenmp -lomp -o fast_subarray fast_subarray.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <chrono>\n",
    "#include <cstdint>\n",
    "#include <limits>\n",
    "#include <iomanip>\n",
    "#include <algorithm>\n",
    "#include <omp.h>\n",
    "\n",
    "// LCG constants from the Python code.\n",
    "constexpr uint64_t LCG_A = 1664525;\n",
    "constexpr uint64_t LCG_C = 1013904223;\n",
    "\n",
    "// LCG function that mimics Python's `(a * value + c) % 2**32`.\n",
    "// Unsigned 32-bit integer arithmetic naturally handles the modulo.\n",
    "inline uint32_t next_lcg(uint32_t& seed) {\n",
    "    seed = static_cast<uint32_t>(LCG_A * seed + LCG_C);\n",
    "    return seed;\n",
    "}\n",
    "\n",
    "// Calculates the maximum subarray sum for n generated numbers.\n",
    "// This uses Kadane's algorithm (O(n)) instead of the O(n^2) brute-force approach.\n",
    "// Numbers are generated on-the-fly to save memory and improve cache usage.\n",
    "long long max_subarray_sum(int n, uint32_t seed, int min_val, int max_val) {\n",
    "    if (n == 0) {\n",
    "        return 0;\n",
    "    }\n",
    "\n",
    "    const long long range = static_cast<long long>(max_val) - min_val + 1;\n",
    "    uint32_t current_seed = seed;\n",
    "\n",
    "    // Generate the first number to initialize the algorithm correctly.\n",
    "    long long first_num = next_lcg(current_seed) % range + min_val;\n",
    "    long long max_so_far = first_num;\n",
    "    long long current_max = first_num;\n",
    "\n",
    "    // Iterate through the rest of the numbers.\n",
    "    for (int i = 1; i < n; ++i) {\n",
    "        long long num = next_lcg(current_seed) % range + min_val;\n",
    "        current_max = std::max(num, current_max + num);\n",
    "        max_so_far = std::max(max_so_far, current_max);\n",
    "    }\n",
    "\n",
    "    return max_so_far;\n",
    "}\n",
    "\n",
    "long long total_max_subarray_sum(int n, uint32_t initial_seed, int min_val, int max_val) {\n",
    "    long long total_sum = 0;\n",
    "    constexpr int num_runs = 20;\n",
    "\n",
    "    // Pre-calculate the 20 seeds. This breaks the sequential dependency and\n",
    "    // allows the 20 runs of max_subarray_sum to be executed in parallel.\n",
    "    std::vector<uint32_t> seeds(num_runs);\n",
    "    uint32_t lcg_state = initial_seed;\n",
    "    for (int i = 0; i < num_runs; ++i) {\n",
    "        seeds[i] = next_lcg(lcg_state);\n",
    "    }\n",
    "\n",
    "    // Use OpenMP to parallelize the 20 independent calculations.\n",
    "    #pragma omp parallel for reduction(+:total_sum)\n",
    "    for (int i = 0; i < num_runs; ++i) {\n",
    "        total_sum += max_subarray_sum(n, seeds[i], min_val, max_val);\n",
    "    }\n",
    "\n",
    "    return total_sum;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Fast I/O\n",
    "    std::ios_base::sync_with_stdio(false);\n",
    "    std::cin.tie(NULL);\n",
    "\n",
    "    // Parameters\n",
    "    constexpr int n = 10000;\n",
    "    constexpr uint32_t initial_seed = 42;\n",
    "    constexpr int min_val = -10;\n",
    "    constexpr int max_val = 10;\n",
    "\n",
    "    auto start_time = std::chrono::high_resolution_clock::now();\n",
    "\n",
    "    long long result = total_max_subarray_sum(n, initial_seed, min_val, max_val);\n",
    "\n",
    "    auto end_time = std::chrono::high_resolution_clock::now();\n",
    "    std::chrono::duration<double> execution_time = end_time - start_time;\n",
    "\n",
    "    std::cout << \"Total Maximum Subarray Sum (20 runs): \" << result << std::endl;\n",
    "    std::cout << \"Execution Time: \" << std::fixed << std::setprecision(6) << execution_time.count() << \" seconds\" << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30f061f3-6e2c-43ce-972e-8822a3d657bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Below is the equivalent Python code that mimics the behavior of the provided C++ code. Note that Python does not have built-in support for OpenMP, but we can use the `concurrent.futures` module to achieve parallelization.\n",
       "\n",
       "```python\n",
       "import time\n",
       "import random\n",
       "from concurrent.futures import ThreadPoolExecutor\n",
       "\n",
       "# LCG constants\n",
       "LCG_A = 1664525\n",
       "LCG_C = 1013904223\n",
       "\n",
       "# LCG function\n",
       "def next_lcg(seed):\n",
       "    seed = (LCG_A * seed + LCG_C) & 0xFFFFFFFF\n",
       "    return seed\n",
       "\n",
       "# Function to calculate the maximum subarray sum using Kadane's algorithm\n",
       "def max_subarray_sum(n, seed, min_val, max_val):\n",
       "    if n == 0:\n",
       "        return 0\n",
       "\n",
       "    range_val = max_val - min_val + 1\n",
       "    current_seed = seed\n",
       "\n",
       "    # Generate the first number\n",
       "    first_num = next_lcg(current_seed) % range_val + min_val\n",
       "    max_so_far = first_num\n",
       "    current_max = first_num\n",
       "\n",
       "    # Iterate through the rest of the numbers\n",
       "    for _ in range(1, n):\n",
       "        num = next_lcg(current_seed) % range_val + min_val\n",
       "        current_max = max(num, current_max + num)\n",
       "        max_so_far = max(max_so_far, current_max)\n",
       "\n",
       "    return max_so_far\n",
       "\n",
       "# Function to calculate the total maximum subarray sum using multiple runs\n",
       "def total_max_subarray_sum(n, initial_seed, min_val, max_val):\n",
       "    total_sum = 0\n",
       "    num_runs = 20\n",
       "\n",
       "    # Pre-calculate the seeds for the 20 runs\n",
       "    seeds = []\n",
       "    lcg_state = initial_seed\n",
       "    for _ in range(num_runs):\n",
       "        seeds.append(next_lcg(lcg_state))\n",
       "\n",
       "    # Use a thread pool to parallelize the 20 independent calculations\n",
       "    with ThreadPoolExecutor(max_workers=num_runs) as executor:\n",
       "        results = executor.map(lambda seed: max_subarray_sum(n, seed, min_val, max_val), seeds)\n",
       "        total_sum = sum(results)\n",
       "\n",
       "    return total_sum\n",
       "\n",
       "def main():\n",
       "    # Parameters\n",
       "    n = 10000\n",
       "    initial_seed = 42\n",
       "    min_val = -10\n",
       "    max_val = 10\n",
       "\n",
       "    start_time = time.time()\n",
       "\n",
       "    result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\n",
       "\n",
       "    end_time = time.time()\n",
       "    execution_time = end_time - start_time\n",
       "\n",
       "    print(f\"Total Maximum Subarray Sum (20 runs): {result}\")\n",
       "    print(f\"Execution Time: {execution_time:.6f} seconds\")\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    main()\n",
       "```\n",
       "\n",
       "### Explanation:\n",
       "1. **LCG Function**: The LCG function `next_lcg` generates the next value in the sequence using the same constants as the C++ code.\n",
       "2. **Max Subarray Sum**: The `max_subarray_sum` function implements Kadane's algorithm to find the maximum subarray sum.\n",
       "3. **Total Max Subarray Sum**: The `total_max_subarray_sum` function generates 20 different seeds and uses a thread pool to parallelize the calculation of the maximum subarray sum for each seed.\n",
       "4. **Main Function**: The `main` function sets the parameters, measures the execution time, and prints the results.\n",
       "\n",
       "This Python code should provide similar performance benefits as the C++ version when running on a multi-core machine, thanks to the parallelization using `ThreadPoolExecutor`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stream finished ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'generator': <generator object OpenRouter._stream_generate at 0x7fb7ff6f1210>,\n",
       " 'text': 'Certainly! Below is the equivalent Python code that mimics the behavior of the provided C++ code. Note that Python does not have built-in support for OpenMP, but we can use the `concurrent.futures` module to achieve parallelization.\\n\\n```python\\nimport time\\nimport random\\nfrom concurrent.futures import ThreadPoolExecutor\\n\\n# LCG constants\\nLCG_A = 1664525\\nLCG_C = 1013904223\\n\\n# LCG function\\ndef next_lcg(seed):\\n    seed = (LCG_A * seed + LCG_C) & 0xFFFFFFFF\\n    return seed\\n\\n# Function to calculate the maximum subarray sum using Kadane\\'s algorithm\\ndef max_subarray_sum(n, seed, min_val, max_val):\\n    if n == 0:\\n        return 0\\n\\n    range_val = max_val - min_val + 1\\n    current_seed = seed\\n\\n    # Generate the first number\\n    first_num = next_lcg(current_seed) % range_val + min_val\\n    max_so_far = first_num\\n    current_max = first_num\\n\\n    # Iterate through the rest of the numbers\\n    for _ in range(1, n):\\n        num = next_lcg(current_seed) % range_val + min_val\\n        current_max = max(num, current_max + num)\\n        max_so_far = max(max_so_far, current_max)\\n\\n    return max_so_far\\n\\n# Function to calculate the total maximum subarray sum using multiple runs\\ndef total_max_subarray_sum(n, initial_seed, min_val, max_val):\\n    total_sum = 0\\n    num_runs = 20\\n\\n    # Pre-calculate the seeds for the 20 runs\\n    seeds = []\\n    lcg_state = initial_seed\\n    for _ in range(num_runs):\\n        seeds.append(next_lcg(lcg_state))\\n\\n    # Use a thread pool to parallelize the 20 independent calculations\\n    with ThreadPoolExecutor(max_workers=num_runs) as executor:\\n        results = executor.map(lambda seed: max_subarray_sum(n, seed, min_val, max_val), seeds)\\n        total_sum = sum(results)\\n\\n    return total_sum\\n\\ndef main():\\n    # Parameters\\n    n = 10000\\n    initial_seed = 42\\n    min_val = -10\\n    max_val = 10\\n\\n    start_time = time.time()\\n\\n    result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\\n\\n    end_time = time.time()\\n    execution_time = end_time - start_time\\n\\n    print(f\"Total Maximum Subarray Sum (20 runs): {result}\")\\n    print(f\"Execution Time: {execution_time:.6f} seconds\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n### Explanation:\\n1. **LCG Function**: The LCG function `next_lcg` generates the next value in the sequence using the same constants as the C++ code.\\n2. **Max Subarray Sum**: The `max_subarray_sum` function implements Kadane\\'s algorithm to find the maximum subarray sum.\\n3. **Total Max Subarray Sum**: The `total_max_subarray_sum` function generates 20 different seeds and uses a thread pool to parallelize the calculation of the maximum subarray sum for each seed.\\n4. **Main Function**: The `main` function sets the parameters, measures the execution time, and prints the results.\\n\\nThis Python code should provide similar performance benefits as the C++ version when running on a multi-core machine, thanks to the parallelization using `ThreadPoolExecutor`.'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.chat_with_stream_display(f\"convert this {cpp} into a python code \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e610bf56-a46e-4aff-8de1-ab49d62b1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import gradio as gr\n",
    "import subprocess\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75022c4d-a2fa-4bf2-a606-344e27cd965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/run/media/sijanpaudel/New Volume/New folder/llm_engineering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38b5a8f0-89d2-49d1-ae01-e82089185a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from key_utils import get_next_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70682f9a-1ebf-439a-a013-daff36b84c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key, _  = get_next_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f672e1c-87e9-4865-b760-370fa605e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "# load_dotenv(override=True)\n",
    "# os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "# os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8aa149ed-9298-4d69-8fe2-8f5de0f667da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "# NOTE - option to use ultra-low cost models by uncommenting last 2 lines\n",
    "\n",
    "# openai = OpenAI()\n",
    "# claude = anthropic.Anthropic()\n",
    "# OPENAI_MODEL = \"gpt-4o\"\n",
    "# CLAUDE_MODEL = \"claude-3-5-sonnet-20240620\"\n",
    "GEMINI_PRO_MODEL = \"gemini-2.5-pro\"\n",
    "# Want to keep costs ultra-low? Uncomment these lines:\n",
    "# OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "# CLAUDE_MODEL = \"claude-3-haiku-20240307\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6896636f-923e-4a2c-9d6c-fac07828a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message =( \"You are an assistant that reimplements Python code in high performance C++ for an M1 Mac. \"\n",
    "\"Respond only with C++ code; use comments sparingly and do not provide any explanation other than occasional comments. \"\n",
    "\"The C++ response needs to produce an identical output in the fastest possible time.\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e7b3546-57aa-4c29-bc5d-f211970d04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(python):\n",
    "    user_prompt = (\"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. \"\n",
    "    \"Respond only with C++ code; do not explain your work other than a few comments. \"\n",
    "    \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\\n\\n\"\n",
    "    f\"{python}\"\n",
    "                  )\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6190659-f54c-4951-bef4-4960f8e51cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def messages_for(python):\n",
    "#     return [\n",
    "#         SystemMessage(content=system_message),\n",
    "#         HumanMessage(content=user_prompt_for(python))\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "71e1ba8c-5b05-4726-a9f3-8d8c6257350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to a file called optimized.cpp\n",
    "\n",
    "def write_output(cpp):\n",
    "    code = cpp.replace(\"```cpp\",\"\").replace(\"```\",\"\")\n",
    "    with open(\"optimized.cpp\", \"w\") as f:\n",
    "        f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b3fa0ac1-a66d-4f81-82d2-6f619a703f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLMHandler import LLMHandler\n",
    "handler_flash = LLMHandler(system_message=system_message)\n",
    "handler_pro = LLMHandler(system_message=system_message)\n",
    "handler_pro.TEXT_MODEL = GEMINI_PRO_MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e0b58799-96d4-4578-ab70-22fe93326895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_gemini_flash(python):\n",
    "    import time\n",
    "    import random\n",
    "    \n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            response = handler_flash.stream_and_process(user_input=user_prompt_for(python))\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
    "            time.sleep((2 ** attempt) + random.random())\n",
    "    write_output(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "af8318d9-c3c3-4c60-bcdd-381262bd0717",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler_pro.TEXT_MODEL =GEMINI_PRO_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ff3fc09e-4399-464d-8389-f323196a227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-2.0-flash\n"
     ]
    }
   ],
   "source": [
    "print(handler_flash.TEXT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "316eec2a-0eef-49f7-b6a9-3cf052bb5f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-2.5-pro\n"
     ]
    }
   ],
   "source": [
    "print(handler_pro.TEXT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4e93b-143c-4a61-89c2-6e42397fe49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5887d-ac9f-4671-b45a-ca59ab0211e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a27e2b-9996-41ae-9e38-61634cf5f516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cdcf54-af41-46b6-aa50-bae4ec22b5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5940119-4717-42a5-806e-38e3f77d72e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a19e494c-5525-4674-a63e-4c87cebaebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_gemini_pro(python):\n",
    "    import time\n",
    "    import random\n",
    "    \n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            response = handler_pro.stream_and_process(user_input=user_prompt_for(python))\n",
    "            \n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
    "            time.sleep((2 ** attempt) + random.random())\n",
    "    write_output(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e7d2fea8-74c6-4421-8f1e-0e76d5b201b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimize_gpt(python):    \n",
    "#     stream = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for(python), stream=True)\n",
    "#     reply = \"\"\n",
    "#     for chunk in stream:\n",
    "#         fragment = chunk.choices[0].delta.content or \"\"\n",
    "#         reply += fragment\n",
    "#         print(fragment, end='', flush=True)\n",
    "#     write_output(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7cd84ad8-d55c-4fe0-9eeb-1895c95c4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimize_claude(python):\n",
    "#     result = claude.messages.stream(\n",
    "#         model=CLAUDE_MODEL,\n",
    "#         max_tokens=2000,\n",
    "#         system=system_message,\n",
    "#         messages=[{\"role\": \"user\", \"content\": user_prompt_for(python)}],\n",
    "#     )\n",
    "#     reply = \"\"\n",
    "#     with result as stream:\n",
    "#         for text in stream.text_stream:\n",
    "#             reply += text\n",
    "#             print(text, end=\"\", flush=True)\n",
    "#     write_output(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a1cbb778-fa57-43de-b04b-ed523f396c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = \"\"\"\n",
    "import time\n",
    "\n",
    "def calculate(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\n",
    "start_time = time.time()\n",
    "result = calculate(100_000_000, 4, 1) * 4\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Result: {result:.12f}\")\n",
    "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7fe1cd4b-d2c5-4303-afed-2115a3fef200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592658589\n",
      "Execution Time: 6.566467 seconds\n"
     ]
    }
   ],
   "source": [
    "exec(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "105db6f9-343c-491d-8e44-3a5328b81719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️ Attempt 1/11 using key from 'pshreesha30'...\n",
      "Assistant is responding...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```cpp\n",
       "#include <iostream>\n",
       "#include <iomanip>\n",
       "#include <chrono>\n",
       "\n",
       "double calculate(long long iterations, int param1, int param2) {\n",
       "    double result = 1.0;\n",
       "    for (long long i = 1; i <= iterations; ++i) {\n",
       "        double j = static_cast<double>(i) * param1 - param2;\n",
       "        result -= (1.0 / j);\n",
       "        j = static_cast<double>(i) * param1 + param2;\n",
       "        result += (1.0 / j);\n",
       "    }\n",
       "    return result;\n",
       "}\n",
       "\n",
       "int main() {\n",
       "    auto start_time = std::chrono::high_resolution_clock::now();\n",
       "    double result = calculate(100000000LL, 4, 1) * 4;\n",
       "    auto end_time = std::chrono::high_resolution_clock::now();\n",
       "\n",
       "    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end_time - start_time);\n",
       "    double execution_time = static_cast<double>(duration.count()) / 1000000.0;\n",
       "\n",
       "    std::cout << std::fixed << std::setprecision(12) << \"Result: \" << result << std::endl;\n",
       "    std::cout << std::fixed << std::setprecision(6) << \"Execution Time: \" << execution_time << \" seconds\" << std::endl;\n",
       "\n",
       "    return 0;\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stream finished ---\n"
     ]
    }
   ],
   "source": [
    "optimize_gemini_flash(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf26ee95-0c77-491d-9a91-579a1e96a8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592658589\n",
      "Execution Time: 7.222917 seconds\n"
     ]
    }
   ],
   "source": [
    "exec(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f8018-f64d-425c-a0e1-d7862aa9592d",
   "metadata": {},
   "source": [
    "# Compiling C++ and executing\n",
    "\n",
    "This next cell contains the command to compile a C++ file on my M1 Mac.  \n",
    "It compiles the file `optimized.cpp` into an executable called `optimized`  \n",
    "Then it runs the program called `optimized`\n",
    "\n",
    "In the next lab (day4), a student has contributed a full solution that compiles to efficient code on Mac, PC and Linux!\n",
    "\n",
    "You can wait for this, or you can google (or ask ChatGPT!) for how to do this on your platform, then replace the lines below.\n",
    "If you're not comfortable with this step, you can skip it for sure - I'll show you exactly how it performs on my Mac.\n",
    "\n",
    "\n",
    "OR alternatively: student Sandeep K.G. points out that you can run Python and C++ code online to test it out that way. Thank you Sandeep!  \n",
    "> Not an exact comparison but you can still get the idea of performance difference.\n",
    "> For example here: https://www.programiz.com/cpp-programming/online-compiler/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4194e40c-04ab-4940-9d64-b4ad37c5bb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592658589\n",
      "Execution Time: 0.180269 seconds\n"
     ]
    }
   ],
   "source": [
    "#Flash\n",
    "!clang++ -O3 -std=c++17 -o optimized optimized.cpp\n",
    "!./optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "983a11fe-e24d-4c65-8269-9802c5ef3ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️ Attempt 1/11 using key from 'nepalcric4'...\n",
      "Assistant is responding...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stream finished ---\n",
      "Attempt 1 failed: 503 The model is overloaded. Please try again later.\n",
      "➡️ Attempt 1/11 using key from 'prabeshsubedi'...\n",
      "Assistant is responding...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```cpp\n",
       "// To compile on M1 Mac with clang for maximum performance:\n",
       "// clang++ -std=c++17 -O3 -march=native -Xpreprocessor -fopenmp -lomp -o fast_calculator fast_calculator.cpp\n",
       "\n",
       "#include <iostream>\n",
       "#include <iomanip>\n",
       "#include <chrono>\n",
       "#include <omp.h>\n",
       "\n",
       "// This function uses an algebraic simplification of the original loop's operations.\n",
       "// Original: result += (1 / (i*p1 + p2)) - (1 / (i*p1 - p2))\n",
       "// Simplified: result += (-2 * p2) / ((i*p1)^2 - p2^2)\n",
       "// This reduces two divisions to one per iteration.\n",
       "double calculate(long long iterations, int param1, int param2) {\n",
       "    double result = 1.0;\n",
       "    const double p2_d = static_cast<double>(param2);\n",
       "    const double neg_2_p2 = -2.0 * p2_d;\n",
       "    const double p2_sq = p2_d * p2_d;\n",
       "\n",
       "    // Parallelize the loop using OpenMP for multi-core processing.\n",
       "    // The 'reduction' clause ensures that the 'result' from each thread\n",
       "    // is correctly summed into the final result.\n",
       "    #pragma omp parallel for reduction(+:result)\n",
       "    for (long long i = 1; i <= iterations; ++i) {\n",
       "        double i_p1_d = static_cast<double>(i) * param1;\n",
       "        result += neg_2_p2 / (i_p1_d * i_p1_d - p2_sq);\n",
       "    }\n",
       "    return result;\n",
       "}\n",
       "\n",
       "int main() {\n",
       "    // Ensure standard library streams are fast.\n",
       "    std::ios_base::sync_with_stdio(false);\n",
       "    std::cin.tie(NULL);\n",
       "\n",
       "    const long long iterations = 100'000'000LL;\n",
       "    const int param1 = 4;\n",
       "    const int param2 = 1;\n",
       "\n",
       "    auto start_time = std::chrono::high_resolution_clock::now();\n",
       "\n",
       "    double result = calculate(iterations, param1, param2) * 4.0;\n",
       "\n",
       "    auto end_time = std::chrono::high_resolution_clock::now();\n",
       "    std::chrono::duration<double> execution_time = end_time - start_time;\n",
       "\n",
       "    std::cout << \"Result: \" << std::fixed << std::setprecision(12) << result << std::endl;\n",
       "    std::cout << \"Execution Time: \" << std::fixed << std::setprecision(6) << execution_time.count() << \" seconds\" << std::endl;\n",
       "\n",
       "    return 0;\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stream finished ---\n"
     ]
    }
   ],
   "source": [
    "optimize_gemini_pro(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d5a766f9-3d23-4bb4-a1d4-88ec44b61ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592662604\n",
      "Execution Time: 0.103814 seconds\n"
     ]
    }
   ],
   "source": [
    "# Repeat for Pro - again, use the right approach for your platform\n",
    "!clang++ -O3 -std=c++17 -o optimized optimized.cpp\n",
    "!./optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c3b497b3-f569-420e-b92e-fb0f49957ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_hard = \"\"\"# Be careful to support large number sizes\n",
    "\n",
    "def lcg(seed, a=1664525, c=1013904223, m=2**32):\n",
    "    value = seed\n",
    "    while True:\n",
    "        value = (a * value + c) % m\n",
    "        yield value\n",
    "        \n",
    "def max_subarray_sum(n, seed, min_val, max_val):\n",
    "    lcg_gen = lcg(seed)\n",
    "    random_numbers = [next(lcg_gen) % (max_val - min_val + 1) + min_val for _ in range(n)]\n",
    "    max_sum = float('-inf')\n",
    "    for i in range(n):\n",
    "        current_sum = 0\n",
    "        for j in range(i, n):\n",
    "            current_sum += random_numbers[j]\n",
    "            if current_sum > max_sum:\n",
    "                max_sum = current_sum\n",
    "    return max_sum\n",
    "\n",
    "def total_max_subarray_sum(n, initial_seed, min_val, max_val):\n",
    "    total_sum = 0\n",
    "    lcg_gen = lcg(initial_seed)\n",
    "    for _ in range(20):\n",
    "        seed = next(lcg_gen)\n",
    "        total_sum += max_subarray_sum(n, seed, min_val, max_val)\n",
    "    return total_sum\n",
    "\n",
    "# Parameters\n",
    "n = 10000         # Number of random numbers\n",
    "initial_seed = 42 # Initial seed for the LCG\n",
    "min_val = -10     # Minimum value of random numbers\n",
    "max_val = 10      # Maximum value of random numbers\n",
    "\n",
    "# Timing the function\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total Maximum Subarray Sum (20 runs):\", result)\n",
    "print(\"Execution Time: {:.6f} seconds\".format(end_time - start_time))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dab5e4bc-276c-4555-bd4c-12c699d5e899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Maximum Subarray Sum (20 runs): 10980\n",
      "Execution Time: 25.137561 seconds\n"
     ]
    }
   ],
   "source": [
    "exec(python_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e8d24ed5-2c15-4f55-80e7-13a3952b3cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️ Attempt 1/11 using key from 'sandeshtiwari'...\n",
      "Assistant is responding...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```cpp\n",
       "#include <iostream>\n",
       "#include <vector>\n",
       "#include <limits>\n",
       "#include <chrono>\n",
       "#include <iomanip>\n",
       "\n",
       "// LCG implementation\n",
       "uint32_t lcg(uint32_t& seed, uint32_t a, uint32_t c, uint64_t m) {\n",
       "    seed = (static_cast<uint64_t>(a) * seed + c) % m;\n",
       "    return seed;\n",
       "}\n",
       "\n",
       "// Max subarray sum\n",
       "long long max_subarray_sum(int n, uint32_t seed, int min_val, int max_val) {\n",
       "    std::vector<int> random_numbers(n);\n",
       "    uint32_t current_seed = seed;\n",
       "    for (int i = 0; i < n; ++i) {\n",
       "        random_numbers[i] = lcg(current_seed, 1664525, 1013904223, static_cast<uint64_t>(1) << 32) % (max_val - min_val + 1) + min_val;\n",
       "    }\n",
       "\n",
       "    long long max_sum = std::numeric_limits<long long>::min();\n",
       "    for (int i = 0; i < n; ++i) {\n",
       "        long long current_sum = 0;\n",
       "        for (int j = i; j < n; ++j) {\n",
       "            current_sum += random_numbers[j];\n",
       "            if (current_sum > max_sum) {\n",
       "                max_sum = current_sum;\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "    return max_sum;\n",
       "}\n",
       "\n",
       "// Total max subarray sum\n",
       "long long total_max_subarray_sum(int n, uint32_t initial_seed, int min_val, int max_val) {\n",
       "    long long total_sum = 0;\n",
       "    uint32_t current_seed = initial_seed;\n",
       "    for (int i = 0; i < 20; ++i) {\n",
       "        uint32_t seed = lcg(current_seed, 1664525, 1013904223, static_cast<uint64_t>(1) << 32);\n",
       "        total_sum += max_subarray_sum(n, seed, min_val, max_val);\n",
       "    }\n",
       "    return total_sum;\n",
       "}\n",
       "\n",
       "int main() {\n",
       "    int n = 10000;\n",
       "    uint32_t initial_seed = 42;\n",
       "    int min_val = -10;\n",
       "    int max_val = 10;\n",
       "\n",
       "    auto start_time = std::chrono::high_resolution_clock::now();\n",
       "    long long result = total_max_subarray_sum(n, initial_seed, min_val, max_val);\n",
       "    auto end_time = std::chrono::high_resolution_clock::now();\n",
       "\n",
       "    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end_time - start_time);\n",
       "    double execution_time = static_cast<double>(duration.count()) / 1000000.0;\n",
       "\n",
       "    std::cout << \"Total Maximum Subarray Sum (20 runs): \" << result << std::endl;\n",
       "    std::cout << std::fixed << std::setprecision(6) << \"Execution Time: \" << execution_time << \" seconds\" << std::endl;\n",
       "\n",
       "    return 0;\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stream finished ---\n"
     ]
    }
   ],
   "source": [
    "optimize_gemini_flash(python_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e0b3d073-88a2-40b2-831c-6f0c345c256f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Maximum Subarray Sum (20 runs): 10980\n",
      "Execution Time: 0.268983 seconds\n"
     ]
    }
   ],
   "source": [
    "# Replace this with the right C++ compile + execute command for your platform\n",
    "\n",
    "!clang++ -O3 -std=c++17 -o optimized optimized.cpp\n",
    "!./optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e9305446-1d0c-4b51-866a-b8c1e299bf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️ Attempt 1/11 using key from 'iamengineer'...\n",
      "Assistant is responding...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stream finished ---\n",
      "Attempt 1 failed: 503 The model is overloaded. Please try again later.\n",
      "➡️ Attempt 1/11 using key from 'birajmohanta'...\n",
      "Assistant is responding...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stream finished ---\n",
      "Attempt 2 failed: 503 The model is overloaded. Please try again later.\n",
      "➡️ Attempt 1/11 using key from 'saharamanandhar'...\n",
      "Assistant is responding...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stream finished ---\n",
      "Attempt 3 failed: 503 The model is overloaded. Please try again later.\n",
      "➡️ Attempt 1/11 using key from 'sijanpaudel'...\n",
      "Assistant is responding...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stream finished ---\n",
      "Attempt 4 failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50\n",
      "Please retry in 6.546221566s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 6\n",
      "}\n",
      "]\n",
      "➡️ Attempt 1/11 using key from 'sijan.paudel10'...\n",
      "Assistant is responding...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```cpp\n",
       "// To compile on M1 Mac with clang for maximum performance:\n",
       "// clang++ -std=c++17 -O3 -march=native -Xpreprocessor -fopenmp -lomp -o fast_subarray fast_subarray.cpp\n",
       "\n",
       "#include <iostream>\n",
       "#include <vector>\n",
       "#include <chrono>\n",
       "#include <cstdint>\n",
       "#include <limits>\n",
       "#include <iomanip>\n",
       "#include <algorithm>\n",
       "#include <omp.h>\n",
       "\n",
       "// LCG constants from the Python code.\n",
       "constexpr uint64_t LCG_A = 1664525;\n",
       "constexpr uint64_t LCG_C = 1013904223;\n",
       "\n",
       "// LCG function that mimics Python's `(a * value + c) % 2**32`.\n",
       "// Unsigned 32-bit integer arithmetic naturally handles the modulo.\n",
       "inline uint32_t next_lcg(uint32_t& seed) {\n",
       "    seed = static_cast<uint32_t>(LCG_A * seed + LCG_C);\n",
       "    return seed;\n",
       "}\n",
       "\n",
       "// Calculates the maximum subarray sum for n generated numbers.\n",
       "// This uses Kadane's algorithm (O(n)) instead of the O(n^2) brute-force approach.\n",
       "// Numbers are generated on-the-fly to save memory and improve cache usage.\n",
       "long long max_subarray_sum(int n, uint32_t seed, int min_val, int max_val) {\n",
       "    if (n == 0) {\n",
       "        return 0;\n",
       "    }\n",
       "\n",
       "    const long long range = static_cast<long long>(max_val) - min_val + 1;\n",
       "    uint32_t current_seed = seed;\n",
       "\n",
       "    // Generate the first number to initialize the algorithm correctly.\n",
       "    long long first_num = next_lcg(current_seed) % range + min_val;\n",
       "    long long max_so_far = first_num;\n",
       "    long long current_max = first_num;\n",
       "\n",
       "    // Iterate through the rest of the numbers.\n",
       "    for (int i = 1; i < n; ++i) {\n",
       "        long long num = next_lcg(current_seed) % range + min_val;\n",
       "        current_max = std::max(num, current_max + num);\n",
       "        max_so_far = std::max(max_so_far, current_max);\n",
       "    }\n",
       "\n",
       "    return max_so_far;\n",
       "}\n",
       "\n",
       "long long total_max_subarray_sum(int n, uint32_t initial_seed, int min_val, int max_val) {\n",
       "    long long total_sum = 0;\n",
       "    constexpr int num_runs = 20;\n",
       "\n",
       "    // Pre-calculate the 20 seeds. This breaks the sequential dependency and\n",
       "    // allows the 20 runs of max_subarray_sum to be executed in parallel.\n",
       "    std::vector<uint32_t> seeds(num_runs);\n",
       "    uint32_t lcg_state = initial_seed;\n",
       "    for (int i = 0; i < num_runs; ++i) {\n",
       "        seeds[i] = next_lcg(lcg_state);\n",
       "    }\n",
       "\n",
       "    // Use OpenMP to parallelize the 20 independent calculations.\n",
       "    #pragma omp parallel for reduction(+:total_sum)\n",
       "    for (int i = 0; i < num_runs; ++i) {\n",
       "        total_sum += max_subarray_sum(n, seeds[i], min_val, max_val);\n",
       "    }\n",
       "\n",
       "    return total_sum;\n",
       "}\n",
       "\n",
       "int main() {\n",
       "    // Fast I/O\n",
       "    std::ios_base::sync_with_stdio(false);\n",
       "    std::cin.tie(NULL);\n",
       "\n",
       "    // Parameters\n",
       "    constexpr int n = 10000;\n",
       "    constexpr uint32_t initial_seed = 42;\n",
       "    constexpr int min_val = -10;\n",
       "    constexpr int max_val = 10;\n",
       "\n",
       "    auto start_time = std::chrono::high_resolution_clock::now();\n",
       "\n",
       "    long long result = total_max_subarray_sum(n, initial_seed, min_val, max_val);\n",
       "\n",
       "    auto end_time = std::chrono::high_resolution_clock::now();\n",
       "    std::chrono::duration<double> execution_time = end_time - start_time;\n",
       "\n",
       "    std::cout << \"Total Maximum Subarray Sum (20 runs): \" << result << std::endl;\n",
       "    std::cout << \"Execution Time: \" << std::fixed << std::setprecision(6) << execution_time.count() << \" seconds\" << std::endl;\n",
       "\n",
       "    return 0;\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stream finished ---\n"
     ]
    }
   ],
   "source": [
    "optimize_gemini_pro(python_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0c181036-8193-4fdd-aef3-fc513b218d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Maximum Subarray Sum (20 runs): 10980\n",
      "Execution Time: 0.000550 seconds\n"
     ]
    }
   ],
   "source": [
    "# Replace this with the right C++ compile + execute command for your platform\n",
    "\n",
    "!clang++ -O3 -std=c++17 -o optimized optimized.cpp\n",
    "!./optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0be9f47d-5213-4700-b0e2-d444c7c738c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gemini_flash(python):    \n",
    "    stream = handler_flash.run(user_prompt_for(python),stream=True)\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk.content or \"\"\n",
    "        reply += fragment\n",
    "        yield reply.replace('```cpp\\n','').replace('```','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8669f56b-8314-4582-a167-78842caea131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gemini_pro(python):\n",
    "    stream = handler_pro.run(user_prompt_for(python),stream=True)\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk.content or \"\"\n",
    "        reply += fragment\n",
    "        yield reply.replace('```cpp\\n','').replace('```','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2f1ae8f5-16c8-40a0-aa18-63b617df078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(python, model):\n",
    "    if model==\"GEMINI_FLASH\":\n",
    "        result = stream_gemini_flash(python)\n",
    "    elif model==\"GEMINI_PRO\":\n",
    "        result = stream_gemini_pro(python)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    for stream_so_far in result:\n",
    "        yield stream_so_far        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ddb38e-6b0a-4c37-baa4-ace0b7de887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        python = gr.Textbox(label=\"Python code:\", lines=10, value=python_hard)\n",
    "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
    "    with gr.Row():\n",
    "        model = gr.Dropdown([\"GEMINI_FLASH\", \"GEMINI_PRO\"], label=\"Select model\", value=\"GEMINI_FLASH\")\n",
    "        convert = gr.Button(\"Convert code\")\n",
    "\n",
    "    convert.click(optimize, inputs=[python, model], outputs=[cpp])\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "19bf2bff-a822-4009-a539-f003b1651383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_python(code):\n",
    "    try:\n",
    "        output = io.StringIO()\n",
    "        sys.stdout = output\n",
    "        exec(code)\n",
    "    finally:\n",
    "        sys.stdout = sys.__stdout__\n",
    "    return output.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "77f3ab5d-fcfb-4d3f-8728-9cacbf833ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll need to change the code in the try block to compile the C++ code for your platform\n",
    "# I pasted this into Claude's chat UI with a request for it to give me a version for an Intel PC,\n",
    "# and it responded with something that looks perfect - you can try a similar approach for your platform.\n",
    "\n",
    "# M1 Mac version to compile and execute optimized C++ code:\n",
    "\n",
    "def execute_cpp(code):\n",
    "        write_output(code)\n",
    "        try:\n",
    "            compile_cmd = [\n",
    "    \"clang++\",\n",
    "    \"-Ofast\",\n",
    "    \"-std=c++17\",\n",
    "    \"-march=native\",   # use CPU of current machine\n",
    "    \"-o\",\n",
    "    \"optimized\",\n",
    "    \"optimized.cpp\"\n",
    "]\n",
    "            compile_result = subprocess.run(compile_cmd, check=True, text=True, capture_output=True)\n",
    "            run_cmd = [\"./optimized\"]\n",
    "            run_result = subprocess.run(run_cmd, check=True, text=True, capture_output=True)\n",
    "            return run_result.stdout\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            return f\"An error occurred:\\n{e.stderr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9a2274f1-d03b-42c0-8dcc-4ce159b18442",
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    ".python {background-color: #306998;}\n",
    ".cpp {background-color: #050;}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f1303932-160c-424b-97a8-d28c816721b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks(css=css) as ui:\n",
    "    gr.Markdown(\"## Convert code from Python to C++\")\n",
    "    with gr.Row():\n",
    "        python = gr.Textbox(label=\"Python code:\", value=python_hard, lines=10)\n",
    "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
    "    with gr.Row():\n",
    "        model = gr.Dropdown([\"GEMINI_FLASH\", \"GEMINI_PRO\"], label=\"Select model\", value=\"GEMINI_FLASH\")\n",
    "    with gr.Row():\n",
    "        convert = gr.Button(\"Convert code\")\n",
    "    with gr.Row():\n",
    "        python_run = gr.Button(\"Run Python\")\n",
    "        cpp_run = gr.Button(\"Run C++\")\n",
    "    with gr.Row():\n",
    "        python_out = gr.TextArea(label=\"Python result:\", elem_classes=[\"python\"])\n",
    "        cpp_out = gr.TextArea(label=\"C++ result:\", elem_classes=[\"cpp\"])\n",
    "\n",
    "    convert.click(optimize, inputs=[python, model], outputs=[cpp])\n",
    "    python_run.click(execute_python, inputs=[python], outputs=[python_out])\n",
    "    cpp_run.click(execute_cpp, inputs=[cpp], outputs=[cpp_out])\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72884cd-bba4-4f5e-94c3-c8abb47eaf1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
