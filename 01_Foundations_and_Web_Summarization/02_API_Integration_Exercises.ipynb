{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# Welcome to your first assignment!\n",
    "\n",
    "Instructions are below. Please give this a try, and look in the solutions folder if you get stuck (or feel free to ask me!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada885d9-4d42-4d9b-97f0-74fbbbfe93a9",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Just before we get to the assignment --</h2>\n",
    "            <span style=\"color:#f71;\">I thought I'd take a second to point you at this page of useful resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458",
   "metadata": {},
   "source": [
    "# HOMEWORK EXERCISE ASSIGNMENT\n",
    "\n",
    "Upgrade the day 1 project to summarize a webpage to use an Open Source model running locally via Ollama rather than OpenAI\n",
    "\n",
    "You'll be able to use this technique for all subsequent projects if you'd prefer not to use paid APIs.\n",
    "\n",
    "**Benefits:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Significantly less power than Frontier Model\n",
    "\n",
    "## Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`.  \n",
    "\n",
    "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
    "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`  \n",
    "Then try [http://localhost:11434/](http://localhost:11434/) again.\n",
    "\n",
    "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b` from a Terminal or Powershell, and change the code below from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ddd15d-a3c5-4f4e-a678-873f56162724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac0a679-599c-441f-9bf2-ddc73d35b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bb9c624-14f0-4945-a719-8ddb64f66f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ff514-e8bd-4985-a572-2ea28bb4fa40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42b9f644-522d-4e05-a691-56e7658c0ea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI (Artificial Intelligence) has numerous business applications across various industries. Here are some examples:\n",
      "\n",
      "1. **Content Generation**: Generative AI can be used to generate high-quality content such as articles, social media posts, product descriptions, and even entire e-books.\n",
      "2. **Visual Content Creation**: Generative AI-powered tools can create stunning visuals like images, videos, and 3D models, which can be used in advertising, marketing, and product design.\n",
      "3. **Customer Service Chatbots**: Generative AI can power chatbots that provide personalized customer support, answering common queries, and routing complex issues to human representatives.\n",
      "4. **Marketing Automation**: Generative AI can help automate marketing processes by generating personalized email campaigns, ad creative, and social media content based on customer behavior and preferences.\n",
      "5. **Product Design**: Generative AI can assist in product design by generating 3D models, CAD designs, and prototypes for new products, reducing the time and cost associated with traditional design methods.\n",
      "6. **Data Analysis and Insights**: Generative AI can analyze large datasets to identify patterns, trends, and correlations, providing actionable insights for businesses.\n",
      "7. **Sales Forecasting**: Generative AI can predict sales performance by analyzing historical data, market trends, and customer behavior, enabling businesses to make informed decisions about inventory management and pricing strategies.\n",
      "8. **Predictive Maintenance**: Generative AI-powered predictive maintenance tools can analyze sensor data from equipment and machinery to predict when maintenance is required, reducing downtime and increasing overall efficiency.\n",
      "9. **Language Translation**: Generative AI-powered translation tools can translate text, speech, and even entire documents, breaking language barriers in global communication.\n",
      "10. **Music and Audio Generation**: Generative AI can create new music tracks, sound effects, or even entire audio dramas, opening up new creative possibilities for music producers, composers, and sound designers.\n",
      "\n",
      "Some specific industries that are leveraging generative AI include:\n",
      "\n",
      "1. **Finance**: Automated risk assessment, portfolio optimization, and investment advice.\n",
      "2. **Healthcare**: Personalized medicine, disease diagnosis, and medical imaging analysis.\n",
      "3. **Manufacturing**: Predictive maintenance, quality control, and product design.\n",
      "4. **Education**: Adaptive learning platforms, personalized tutoring, and AI-powered grading systems.\n",
      "5. **Retail**: Automated customer service chatbots, product recommendations, and supply chain optimization.\n",
      "\n",
      "These are just a few examples of the many business applications of generative AI. As the technology continues to evolve, we can expect to see even more innovative uses in various industries.\n"
     ]
    }
   ],
   "source": [
    "# If this doesn't work for any reason, try the 2 versions in the following cells\n",
    "# And double check the instructions in the 'Recap on installation of Ollama' at the top of this lab\n",
    "# And if none of that works - contact me!\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da3502-ca8e-45ef-a21c-fbeaf2e4d7d9",
   "metadata": {},
   "source": [
    "## Using LangChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f14f1b3a-83ea-4d8a-b24d-4b78418cde97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "model = ChatOllama(model=MODEL)\n",
    "messages = [\n",
    "    SystemMessage(content =\"Give the output in proper coding style, with each code in single line \" ),\n",
    "    HumanMessage(content = \"Write a C code to check palindrome\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8566c71a-20c5-4426-8f7e-196632c5d774",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here is a simple C program that checks whether a given number is a palindrome or not:\\n\\n```c\\n#include <stdio.h>\\n\\nint main() {\\n    int num;\\n    printf(\"Enter a number: \");\\n    scanf(\"%d\", &num);\\n\\n    int rev = 0, orig = num;\\n    while (num != 0) {\\n        int rem = num % 10;\\n        rev = (rev * 10) + rem;\\n        num /= 10;\\n    }\\n\\n    if (orig == rev)\\n        printf(\"%d is a palindrome.\\\\n\", orig);\\n    else\\n        printf(\"%d is not a palindrome.\\\\n\", orig);\\n\\n    return 0;\\n}\\n```\\n\\nThis code will first reverse the given number and then compare it with the original number. If they are equal, the number is a palindrome; otherwise, it\\'s not.\\n\\nAlternatively, you can use recursion to check if a number is a palindrome:\\n\\n```c\\n#include <stdio.h>\\n\\nint is_palindrome(int num) {\\n    int rev = 0;\\n    while (num != 0) {\\n        int rem = num % 10;\\n        rev = (rev * 10) + rem;\\n        num /= 10;\\n    }\\n    return orig == rev; // original number for palindrome check\\n}\\n\\nint main() {\\n    int num;\\n    printf(\"Enter a number: \");\\n    scanf(\"%d\", &num);\\n\\n    if (is_palindrome(num))\\n        printf(\"%d is a palindrome.\\\\n\", num);\\n    else\\n        printf(\"%d is not a palindrome.\\\\n\", num);\\n\\n    return 0;\\n}\\n```\\n\\nIn this version, the `is_palindrome` function returns 1 (true) if the number is a palindrome and 0 (false) otherwise. The `main` function calls `is_palindrome` to perform the check.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-09-16T10:51:11.267346286Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6165591650, 'load_duration': 67591911, 'prompt_eval_count': 47, 'prompt_eval_duration': 22437619, 'eval_count': 382, 'eval_duration': 6074435464, 'model_name': 'llama3.2'}, id='run--31052335-b511-4dc9-b283-a2edd883356d-0', usage_metadata={'input_tokens': 47, 'output_tokens': 382, 'total_tokens': 429})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response= model.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dcac8b6-934c-413b-90c6-30b487517971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is a simple C program that checks whether a given number is a palindrome or not:\n",
       "\n",
       "```c\n",
       "#include <stdio.h>\n",
       "\n",
       "int main() {\n",
       "    int num;\n",
       "    printf(\"Enter a number: \");\n",
       "    scanf(\"%d\", &num);\n",
       "\n",
       "    int rev = 0, orig = num;\n",
       "    while (num != 0) {\n",
       "        int rem = num % 10;\n",
       "        rev = (rev * 10) + rem;\n",
       "        num /= 10;\n",
       "    }\n",
       "\n",
       "    if (orig == rev)\n",
       "        printf(\"%d is a palindrome.\\n\", orig);\n",
       "    else\n",
       "        printf(\"%d is not a palindrome.\\n\", orig);\n",
       "\n",
       "    return 0;\n",
       "}\n",
       "```\n",
       "\n",
       "This code will first reverse the given number and then compare it with the original number. If they are equal, the number is a palindrome; otherwise, it's not.\n",
       "\n",
       "Alternatively, you can use recursion to check if a number is a palindrome:\n",
       "\n",
       "```c\n",
       "#include <stdio.h>\n",
       "\n",
       "int is_palindrome(int num) {\n",
       "    int rev = 0;\n",
       "    while (num != 0) {\n",
       "        int rem = num % 10;\n",
       "        rev = (rev * 10) + rem;\n",
       "        num /= 10;\n",
       "    }\n",
       "    return orig == rev; // original number for palindrome check\n",
       "}\n",
       "\n",
       "int main() {\n",
       "    int num;\n",
       "    printf(\"Enter a number: \");\n",
       "    scanf(\"%d\", &num);\n",
       "\n",
       "    if (is_palindrome(num))\n",
       "        printf(\"%d is a palindrome.\\n\", num);\n",
       "    else\n",
       "        printf(\"%d is not a palindrome.\\n\", num);\n",
       "\n",
       "    return 0;\n",
       "}\n",
       "```\n",
       "\n",
       "In this version, the `is_palindrome` function returns 1 (true) if the number is a palindrome and 0 (false) otherwise. The `main` function calls `is_palindrome` to perform the check."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.content))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ca09a3f-b111-4c15-9119-aafd3db853da",
   "metadata": {},
   "source": [
    "\n",
    "# Introducing the ollama package\n",
    "\n",
    "And now we'll do the same thing\n",
    ", but using the elegant ollama python package instead of a direct HTTP call.\n",
    "\n",
    "Under the hood, it's making the same call as above to the ollama server running at localhost:11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745b9c4-57dc-4867-9180-61fa5db55eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4704e10-f5fb-4c15-a935-f046c06fb13d",
   "metadata": {},
   "source": [
    "## Alternative approach - using OpenAI python library to connect to Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23057e00-b6fc-4678-93a9-6b31cb704bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's actually an alternative approach that some people might prefer\n",
    "# You can use the OpenAI client python library to call Ollama:\n",
    "\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e22da-b891-41f6-9ac9-bd0c0a5f4f44",
   "metadata": {},
   "source": [
    "## Are you confused about why that works?\n",
    "\n",
    "It seems strange, right? We just used OpenAI code to call Ollama?? What's going on?!\n",
    "\n",
    "Here's the scoop:\n",
    "\n",
    "The python class `OpenAI` is simply code written by OpenAI engineers that makes calls over the internet to an endpoint.  \n",
    "\n",
    "When you call `openai.chat.completions.create()`, this python code just makes a web request to the following url: \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "Code like this is known as a \"client library\" - it's just wrapper code that runs on your machine to make web requests. The actual power of GPT is running on OpenAI's cloud behind this API, not on your computer!\n",
    "\n",
    "OpenAI was so popular, that lots of other AI providers provided identical web endpoints, so you could use the same approach.\n",
    "\n",
    "So Ollama has an endpoint running on your local box at http://localhost:11434/v1/chat/completions  \n",
    "And in week 2 we'll discover that lots of other providers do this too, including Gemini and DeepSeek.\n",
    "\n",
    "And then the team at OpenAI had a great idea: they can extend their client library so you can specify a different 'base url', and use their library to call any compatible API.\n",
    "\n",
    "That's it!\n",
    "\n",
    "So when you say: `ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')`  \n",
    "Then this will make the same endpoint calls, but to Ollama instead of OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d1de3-e2ac-46ff-a302-3b4ba38c4c90",
   "metadata": {},
   "source": [
    "## Also trying the amazing reasoning model DeepSeek\n",
    "\n",
    "Here we use the version of DeepSeek-reasoner that's been distilled to 1.5B.  \n",
    "This is actually a 1.5B variant of Qwen that has been fine-tuned using synethic data generated by Deepseek R1.\n",
    "\n",
    "Other sizes of DeepSeek are [here](https://ollama.com/library/deepseek-r1) all the way up to the full 671B parameter version, which would use up 404GB of your drive and is far too large for most!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9eb44e-fe5b-47aa-b719-0bb63669ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d554b-e00d-4c08-9300-45e073950a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a few minutes to run! You should then see a fascinating \"thinking\" trace inside <think> tags, followed by some decent definitions\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Please give definitions of some core concepts behind LLMs: a neural network, attention and the transformer\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622d9bb-5c68-4d4e-9ca4-b492c751f898",
   "metadata": {},
   "source": [
    "# NOW the exercise for you\n",
    "\n",
    "Take the code from day1 and incorporate it here, to build a website summarizer that uses Llama 3.2 running locally instead of OpenAI; use either of the above approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de38216-6d1c-48c4-877b-86d403f4e0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
